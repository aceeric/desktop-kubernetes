
NAME: [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
FAILURE: /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb 28 17:18:00.206: Session is sticky after reaching the timeout
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3395
SYSTEM-OUT: [BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb 28 17:14:29.204: INFO: >>> kubeConfig: /tmp/kubeconfig-913705716
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7795
Feb 28 17:14:31.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 28 17:14:31.375: INFO: rc: 7
Feb 28 17:14:31.386: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 28 17:14:31.389: INFO: Pod kube-proxy-mode-detector no longer exists
Feb 28 17:14:31.389: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-7795
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7795
Feb 28 17:14:34.472: INFO: Creating new exec pod
Feb 28 17:14:37.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Feb 28 17:14:37.647: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80
Connection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!
"
Feb 28 17:14:37.647: INFO: stdout: ""
Feb 28 17:14:37.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.2 80'
Feb 28 17:14:37.788: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.2 80
Connection to 10.32.0.2 80 port [tcp/http] succeeded!
"
Feb 28 17:14:37.789: INFO: stdout: ""
Feb 28 17:14:37.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.221 31453'
Feb 28 17:14:37.885: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.221 31453
Connection to 192.168.0.221 31453 port [tcp/31453] succeeded!
"
Feb 28 17:14:37.885: INFO: stdout: ""
Feb 28 17:14:37.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.220 31453'
Feb 28 17:14:37.972: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.220 31453
Connection to 192.168.0.220 31453 port [tcp/31453] succeeded!
"
Feb 28 17:14:37.972: INFO: stdout: ""
Feb 28 17:14:37.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.168:31453/ ; done'
Feb 28 17:14:38.116: INFO: stderr: "+ seq 0 15
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
+ echo
+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:14:38.116: INFO: stdout: "
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9
affinity-nodeport-timeout-sltl9"
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Received response from host: affinity-nodeport-timeout-sltl9
Feb 28 17:14:38.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:14:38.302: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:14:38.302: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:14:58.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:14:58.410: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:14:58.410: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:15:18.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:15:18.534: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:15:18.534: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:15:38.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:15:38.706: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:15:38.706: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:15:58.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:15:58.847: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:15:58.847: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:16:18.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:16:18.989: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:16:18.989: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:16:38.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:16:39.762: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:16:39.762: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:16:59.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:16:59.927: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:16:59.927: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:17:19.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:17:20.060: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:17:20.060: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:17:40.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-913705716 --namespace=services-7795 exec execpod-affinity4nddw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.168:31453/'
Feb 28 17:17:40.201: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.168:31453/
"
Feb 28 17:17:40.201: INFO: stdout: "affinity-nodeport-timeout-sltl9"
Feb 28 17:18:00.206: FAIL: Session is sticky after reaching the timeout

Full Stack Trace
k8s.io/kubernetes/test/e2e/network.execAffinityTestForSessionAffinityTimeout(0xc000c04f20, 0x55eb720, 0xc0019f2dc0, 0xc00339ea00)
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:3395 +0xc96
k8s.io/kubernetes/test/e2e/network.glob..func24.29()
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:2469 +0x9c
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc0027de600)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x36c
k8s.io/kubernetes/test/e2e.TestE2E(0xc0027de600)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x2b
testing.tRunner(0xc0027de600, 0x4fa8cc8)
	/usr/local/go/src/testing/testing.go:1123 +0xef
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1168 +0x2b3
Feb 28 17:18:00.208: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7795, will wait for the garbage collector to delete the pods
Feb 28 17:18:00.296: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.978652ms
Feb 28 17:18:01.297: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.000268925s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "services-7795".
STEP: Found 34 events.
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:29 +0000 UTC - event for kube-proxy-mode-detector: {default-scheduler } Scheduled: Successfully assigned services-7795/kube-proxy-mode-detector to ham
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:29 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ham} DNSConfigForming: Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 75.75.75.75 75.75.76.76 2001:558:feed::1
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:29 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ham} Started: Started container agnhost-container
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:29 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ham} Created: Created container agnhost-container
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:29 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ham} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.21" already present on machine
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-7vvvd
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-9wvvq
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout: {replication-controller } SuccessfulCreate: Created pod: affinity-nodeport-timeout-sltl9
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout-7vvvd: {kubelet ham} DNSConfigForming: Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 75.75.75.75 75.75.76.76 2001:558:feed::1
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout-7vvvd: {default-scheduler } Scheduled: Successfully assigned services-7795/affinity-nodeport-timeout-7vvvd to ham
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout-9wvvq: {default-scheduler } Scheduled: Successfully assigned services-7795/affinity-nodeport-timeout-9wvvq to doc
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout-9wvvq: {kubelet doc} DNSConfigForming: Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 75.75.75.75 75.75.76.76 2001:558:feed::1
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout-sltl9: {default-scheduler } Scheduled: Successfully assigned services-7795/affinity-nodeport-timeout-sltl9 to monk
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for affinity-nodeport-timeout-sltl9: {kubelet monk} DNSConfigForming: Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 75.75.75.75 75.75.76.76 2001:558:feed::1
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ham} Killing: Stopping container agnhost-container
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:31 +0000 UTC - event for kube-proxy-mode-detector: {kubelet ham} SandboxChanged: Pod sandbox changed, it will be killed and re-created.
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-7vvvd: {kubelet ham} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.21" already present on machine
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-7vvvd: {kubelet ham} Started: Started container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-7vvvd: {kubelet ham} Created: Created container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-9wvvq: {kubelet doc} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.21" already present on machine
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-9wvvq: {kubelet doc} Started: Started container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-9wvvq: {kubelet doc} Created: Created container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-sltl9: {kubelet monk} Created: Created container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-sltl9: {kubelet monk} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.21" already present on machine
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:32 +0000 UTC - event for affinity-nodeport-timeout-sltl9: {kubelet monk} Started: Started container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:34 +0000 UTC - event for execpod-affinity4nddw: {default-scheduler } Scheduled: Successfully assigned services-7795/execpod-affinity4nddw to doc
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:35 +0000 UTC - event for execpod-affinity4nddw: {kubelet doc} Pulled: Container image "k8s.gcr.io/e2e-test-images/agnhost:2.21" already present on machine
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:35 +0000 UTC - event for execpod-affinity4nddw: {kubelet doc} Created: Created container agnhost-container
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:14:35 +0000 UTC - event for execpod-affinity4nddw: {kubelet doc} Started: Started container agnhost-container
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:18:00 +0000 UTC - event for execpod-affinity4nddw: {kubelet doc} Killing: Stopping container agnhost-container
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:18:01 +0000 UTC - event for affinity-nodeport-timeout: {endpoint-controller } FailedToUpdateEndpoint: Failed to update endpoint services-7795/affinity-nodeport-timeout: Operation cannot be fulfilled on endpoints "affinity-nodeport-timeout": the object has been modified; please apply your changes to the latest version and try again
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:18:01 +0000 UTC - event for affinity-nodeport-timeout-7vvvd: {kubelet ham} Killing: Stopping container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:18:01 +0000 UTC - event for affinity-nodeport-timeout-9wvvq: {kubelet doc} Killing: Stopping container affinity-nodeport-timeout
Feb 28 17:18:14.234: INFO: At 2021-02-28 17:18:01 +0000 UTC - event for affinity-nodeport-timeout-sltl9: {kubelet monk} Killing: Stopping container affinity-nodeport-timeout
Feb 28 17:18:14.236: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Feb 28 17:18:14.236: INFO:
Feb 28 17:18:14.239: INFO:
Logging node info for node doc
Feb 28 17:18:14.240: INFO: Node Info: &Node{ObjectMeta:{doc    a469f9c8-4a51-4ca3-9d5e-d56426522e3a 62815 0 2021-02-27 18:37:07 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:doc kubernetes.io/os:linux node-role.kubernetes.io/controller: node-role.kubernetes.io/worker:] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-02-27 18:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.200.0.0/24\"":{}}}}} {kubectl Update v1 2021-02-27 18:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:node-role.kubernetes.io/controller":{},"f:node-role.kubernetes.io/worker":{}}}}} {kubelet Update v1 2021-02-28 17:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.200.0.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.200.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{17785946112 0} {<nil>} 16962Mi BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8145018880 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{16007351475 0} {<nil>} 16007351475 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8040161280 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-27 18:37:07 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-27 18:37:07 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-27 18:37:07 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:23 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:192.168.0.168,},NodeAddress{Type:Hostname,Address:doc,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:cf27580027864897b10943e217a56fc8,SystemUUID:0da957dd-8389-f747-b18a-e7b6f5012201,BootID:a4ff37c6-f02d-460c-bb34-5a5937acb086,KernelVersion:4.18.0-240.10.1.el8_3.x86_64,OSImage:CentOS Linux 8,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.1,KubeProxyVersion:v1.20.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/strimzi/kafka@sha256:7d383a11a6466f15f8a3c10010406e76fec1493adfc97f08c9d58ef2c90cade5 quay.io/strimzi/kafka:0.21.1-kafka-2.7.0],SizeBytes:315001619,},ContainerImage{Names:[quay.io/strimzi/operator@sha256:5d3f4313b465b6a987b0de398969342b31157cacde1b06ed9b5f2e7d08e73549 quay.io/strimzi/operator:0.21.1],SizeBytes:230081580,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:9f165f5d6393246eae4ea77cfc99d7f39b7475bd7650c72759d44f1f2cd3bafe k8s.gcr.io/conformance:v1.20.1],SizeBytes:70202323,},ContainerImage{Names:[docker.io/kubernetesui/dashboard@sha256:06868692fb9a7f2ede1a06de1b7b32afabc40ec739c1181d83b5ed3eb147ec6e docker.io/kubernetesui/dashboard:v2.0.0],SizeBytes:66209190,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[docker.io/cloudnativelabs/kube-router@sha256:75be3834e61c184ffed0adbdcc9e168a75e1b4f28a1540d4aa377af36259bfc9 docker.io/cloudnativelabs/kube-router:latest],SizeBytes:39212170,},ContainerImage{Names:[projects.registry.vmware.com/sonobuoy/sonobuoy@sha256:ba5c473283dbadcbc56be1149ae45d60df020260ba851e3fc28acc445b300a54 projects.registry.vmware.com/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608089,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server@sha256:dbc33d7d35d2a9cc5ab402005aa7a0d13be6192f3550c7d42cba8d2d5e3a5d62 k8s.gcr.io/metrics-server/metrics-server:v0.4.2],SizeBytes:25168493,},ContainerImage{Names:[docker.io/kubernetesui/metrics-scraper@sha256:555981a24f184420f3be0c79d4efb6c948a85cfce84034f85a563f4151a81cbf docker.io/kubernetesui/metrics-scraper:v1.0.4],SizeBytes:16020077,},ContainerImage{Names:[docker.io/coredns/coredns@sha256:cc8fb77bc2a0541949d1d9320a641b82fd392b0d3d8145469ca4709ae769980e docker.io/coredns/coredns:1.8.0],SizeBytes:12945155,},ContainerImage{Names:[docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker.io/library/nginx:1.14-alpine],SizeBytes:6978806,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:1804628,},ContainerImage{Names:[docker.io/library/busybox@sha256:c6b45a95f932202dbb27c31333c4789f45184a744060f6e569cc9d2bf1b9ad6f docker.io/library/busybox:latest],SizeBytes:768719,},ContainerImage{Names:[docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/library/busybox:1.29],SizeBytes:732685,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:299513,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Feb 28 17:18:14.240: INFO:
Logging kubelet events for node doc
Feb 28 17:18:14.242: INFO:
Logging pods the kubelet thinks is on node doc
Feb 28 17:18:14.260: INFO: metrics-server-76f8d9fc69-dmjk9 started at 2021-02-27 18:43:25 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Container metrics-server ready: true, restart count 1
Feb 28 17:18:14.260: INFO: my-cluster-entity-operator-5fd974964d-2qvzp started at 2021-02-28 02:31:34 +0000 UTC (0+3 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Container tls-sidecar ready: true, restart count 1
Feb 28 17:18:14.260: INFO: 	Container topic-operator ready: true, restart count 3
Feb 28 17:18:14.260: INFO: 	Container user-operator ready: true, restart count 2
Feb 28 17:18:14.260: INFO: strimzi-cluster-operator-68c6747bc6-w8gkr started at 2021-02-28 02:22:32 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Container strimzi-cluster-operator ready: true, restart count 1
Feb 28 17:18:14.260: INFO: sonobuoy-e2e-job-019aa6e1d9954680 started at 2021-02-28 15:54:13 +0000 UTC (0+2 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Container e2e ready: true, restart count 0
Feb 28 17:18:14.260: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 28 17:18:14.260: INFO: kube-router-89ll8 started at 2021-02-27 18:43:04 +0000 UTC (1+1 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Init container install-cni ready: true, restart count 1
Feb 28 17:18:14.260: INFO: 	Container kube-router ready: true, restart count 1
Feb 28 17:18:14.260: INFO: coredns-75c4c67f76-kwhcs started at 2021-02-27 18:43:15 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Container coredns ready: true, restart count 1
Feb 28 17:18:14.260: INFO: kubernetes-dashboard-74d688b6bc-mnvvm started at 2021-02-27 18:43:25 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Feb 28 17:18:14.260: INFO: dashboard-metrics-scraper-7b59f7d4df-p9jmd started at 2021-02-27 18:43:25 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.260: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 1
Feb 28 17:18:14.312: INFO:
Latency metrics for node doc
Feb 28 17:18:14.312: INFO:
Logging node info for node ham
Feb 28 17:18:14.314: INFO: Node Info: &Node{ObjectMeta:{ham    d078a37f-3a37-4cfc-98be-23137945ca69 62814 0 2021-02-27 18:40:04 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:ham kubernetes.io/os:linux node-role.kubernetes.io/worker:] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubectl Update v1 2021-02-27 18:40:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:node-role.kubernetes.io/worker":{}}}}} {kube-controller-manager Update v1 2021-02-28 04:21:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.200.1.0/24\"":{}}}}} {e2e.test Update v1 2021-02-28 16:54:56 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{}}}}} {kubelet Update v1 2021-02-28 17:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.200.1.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.200.1.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{17785946112 0} {<nil>} 16962Mi BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8145461248 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{16007351475 0} {<nil>} 16007351475 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8040603648 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:22 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:22 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:22 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:22 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:192.168.0.220,},NodeAddress{Type:Hostname,Address:ham,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:cf27580027864897b10943e217a56fc8,SystemUUID:fcf0d19e-b7b3-0248-b37e-0f37973dbf72,BootID:6736e26d-03a9-46d9-95ea-4d18f940d8fb,KernelVersion:4.18.0-240.10.1.el8_3.x86_64,OSImage:CentOS Linux 8,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.1,KubeProxyVersion:v1.20.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/strimzi/kafka@sha256:7d383a11a6466f15f8a3c10010406e76fec1493adfc97f08c9d58ef2c90cade5 quay.io/strimzi/kafka:0.21.1-kafka-2.7.0],SizeBytes:315001619,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:85425365,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[docker.io/cloudnativelabs/kube-router@sha256:75be3834e61c184ffed0adbdcc9e168a75e1b4f28a1540d4aa377af36259bfc9 docker.io/cloudnativelabs/kube-router:latest],SizeBytes:39212170,},ContainerImage{Names:[projects.registry.vmware.com/sonobuoy/sonobuoy@sha256:ba5c473283dbadcbc56be1149ae45d60df020260ba851e3fc28acc445b300a54 projects.registry.vmware.com/sonobuoy/sonobuoy:v0.20.0],SizeBytes:36608089,},ContainerImage{Names:[docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker.io/library/nginx:1.14-alpine],SizeBytes:6978806,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:1804628,},ContainerImage{Names:[docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/library/busybox:1.29],SizeBytes:732685,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:299513,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Feb 28 17:18:14.314: INFO:
Logging kubelet events for node ham
Feb 28 17:18:14.316: INFO:
Logging pods the kubelet thinks is on node ham
Feb 28 17:18:14.330: INFO: sonobuoy started at 2021-02-28 15:54:13 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.330: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 28 17:18:14.330: INFO: my-cluster-zookeeper-0 started at 2021-02-28 16:55:40 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.330: INFO: 	Container zookeeper ready: true, restart count 0
Feb 28 17:18:14.330: INFO: kube-router-h4jhc started at 2021-02-27 18:43:04 +0000 UTC (1+1 container statuses recorded)
Feb 28 17:18:14.330: INFO: 	Init container install-cni ready: true, restart count 1
Feb 28 17:18:14.330: INFO: 	Container kube-router ready: true, restart count 1
Feb 28 17:18:14.359: INFO:
Latency metrics for node ham
Feb 28 17:18:14.359: INFO:
Logging node info for node monk
Feb 28 17:18:14.361: INFO: Node Info: &Node{ObjectMeta:{monk    648f9955-9510-4ce9-a461-9fd80a036f2d 62813 0 2021-02-27 18:43:01 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:monk kubernetes.io/os:linux node-role.kubernetes.io/worker:] map[node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubectl Update v1 2021-02-27 18:43:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:node-role.kubernetes.io/worker":{}}}}} {kube-controller-manager Update v1 2021-02-28 04:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.200.2.0/24\"":{}}}}} {kubelet Update v1 2021-02-28 17:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.200.2.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.200.2.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{17785946112 0} {<nil>} 16962Mi BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8145461248 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{16007351475 0} {<nil>} 16007351475 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8040603648 0} {<nil>}  BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:24 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:24 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:24 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-02-28 17:16:38 +0000 UTC,LastTransitionTime:2021-02-28 15:45:24 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:192.168.0.221,},NodeAddress{Type:Hostname,Address:monk,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:cf27580027864897b10943e217a56fc8,SystemUUID:5c73620c-4a25-d542-9e3c-faacfaeefe60,BootID:aaab4e66-7c7d-4f9e-8b2d-c414b863fd0b,KernelVersion:4.18.0-240.10.1.el8_3.x86_64,OSImage:CentOS Linux 8,ContainerRuntimeVersion:containerd://1.4.3,KubeletVersion:v1.20.1,KubeProxyVersion:v1.20.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/strimzi/kafka@sha256:7d383a11a6466f15f8a3c10010406e76fec1493adfc97f08c9d58ef2c90cade5 quay.io/strimzi/kafka:0.21.1-kafka-2.7.0],SizeBytes:315001619,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a k8s.gcr.io/e2e-test-images/agnhost:2.21],SizeBytes:46251468,},ContainerImage{Names:[docker.io/library/httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a docker.io/library/httpd:2.4.39-alpine],SizeBytes:41901429,},ContainerImage{Names:[docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:40765017,},ContainerImage{Names:[docker.io/cloudnativelabs/kube-router@sha256:75be3834e61c184ffed0adbdcc9e168a75e1b4f28a1540d4aa377af36259bfc9 docker.io/cloudnativelabs/kube-router:latest],SizeBytes:39212170,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:25311280,},ContainerImage{Names:[docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker.io/library/nginx:1.14-alpine],SizeBytes:6978806,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs@sha256:10066e9039219449fe3c81f38fe01928f87914150768ab81b62a468e51fa7411 gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:3054649,},ContainerImage{Names:[docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 docker.io/library/busybox:1.29],SizeBytes:732685,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:299513,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Feb 28 17:18:14.361: INFO:
Logging kubelet events for node monk
Feb 28 17:18:14.362: INFO:
Logging pods the kubelet thinks is on node monk
Feb 28 17:18:14.376: INFO: my-cluster-kafka-exporter-7975fdd4b7-w9hl2 started at 2021-02-28 16:33:28 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.376: INFO: 	Container my-cluster-kafka-exporter ready: true, restart count 0
Feb 28 17:18:14.376: INFO: my-cluster-kafka-0 started at 2021-02-28 15:55:34 +0000 UTC (0+1 container statuses recorded)
Feb 28 17:18:14.376: INFO: 	Container kafka ready: true, restart count 0
Feb 28 17:18:14.376: INFO: kube-router-rgwlb started at 2021-02-27 18:43:04 +0000 UTC (1+1 container statuses recorded)
Feb 28 17:18:14.376: INFO: 	Init container install-cni ready: true, restart count 1
Feb 28 17:18:14.376: INFO: 	Container kube-router ready: true, restart count 1
Feb 28 17:18:14.407: INFO:
Latency metrics for node monk
Feb 28 17:18:14.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7795" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

