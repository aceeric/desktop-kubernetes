#!/usr/bin/env bash

set -e

long_opts=help,host-network-interface:,vboxdir:,check-compatibility,from-scratch,create-template,networking:,monitoring:,\
single-node,up:,down:,delete:
script_name=$(basename "$0")
proj_root="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
priv_key=$proj_root/kickstart/id_ed25519
admin_kubeconfig=$proj_root/admin/admin.kubeconfig

# api
host_network_interface=
vboxdir=
check_compatibility=0
from_scratch=0
create_template=0
networking=
monitoring=
single_node=0
up=
down=
delete=

# internal
downloading_msg=""
cluster_vms=(doc)

# These are the various binaries and manifests that are downloaded by the scripts in the project. Two
# elements are defined for each upstream: the URL to get it from, and the filesystem location. This way,
# once the script is run "from scratch", subsequent invocations can just use the downloaded objects. ISOs
# go in the 'media' directory, and everything else goes in the 'binaries' directory. The kickstart ISO
# is not downloaded, it is generated by the project. All these objects are git-ignored

centos_iso_download=http://mirror.umd.edu/centos/8.3.2011/isos/x86_64/CentOS-8.3.2011-x86_64-dvd1.iso
centos_iso_path=$proj_root/media/CentOS-8.3.2011-x86_64-dvd1.iso
guest_additions_download=http://download.virtualbox.org/virtualbox/6.1.18/VBoxGuestAdditions_6.1.18.iso
guest_additions_path=$proj_root/media/VBoxGuestAdditions_6.1.18.iso
etcd_download=https://github.com/etcd-io/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz
etcd_gzip=$proj_root/binaries/etcd-v3.4.14-linux-amd64.tar.gz
kube_apiserver_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kube-apiserver
kube_apiserver_binary=$proj_root/binaries/kube-apiserver
kube_controller_manager_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kube-controller-manager
kube_controller_manager_binary=$proj_root/binaries/kube-controller-manager
kube_scheduler_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kube-scheduler
kube_scheduler_binary=$proj_root/binaries/kube-scheduler
crictl_download=https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.19.0/crictl-v1.19.0-linux-amd64.tar.gz
crictl_binary=$proj_root/binaries/crictl-v1.19.0-linux-amd64.tar.gz
runc_download=https://github.com/opencontainers/runc/releases/download/v1.0.0-rc92/runc.amd64
runc_binary=$proj_root/binaries/runc
cni_plugins_download=https://github.com/containernetworking/plugins/releases/download/v0.9.0/cni-plugins-linux-amd64-v0.9.0.tgz
cni_plugins_binary=$proj_root/binaries/cni-plugins-linux-amd64-v0.9.0.tgz
containerd_download=https://github.com/containerd/containerd/releases/download/v1.4.3/containerd-1.4.3-linux-amd64.tar.gz
containerd_binary=$proj_root/binaries/containerd-1.4.3-linux-amd64.tar.gz
kubelet_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kubelet
kubelet_binary=$proj_root/binaries/kubelet
kube_router_yaml_download=https://raw.githubusercontent.com/cloudnativelabs/kube-router/v1.1.1/daemonset/generic-kuberouter-all-features.yaml
kube_router_yaml=$proj_root/binaries/generic-kuberouter-all-features.yaml
metrics_server_download=https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.2/components.yaml
metrics_server_manifest=$proj_root/binaries/metrics-server-components.yaml
kubernetes_dashboard_download=https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
kubernetes_dashboard_manifest=$proj_root/binaries/kubernetes-dashboard-recommended.yaml
# do cilium/hubble differently - see the README with the install script
#cilium_yaml_download=https://raw.githubusercontent.com/cilium/cilium/1.9.4/install/kubernetes/quick-install.yaml
#cilium_yaml=cilium-1.9.4-quick-install.yaml
#hubble_yaml_download=https://raw.githubusercontent.com/cilium/cilium/1.9.4/install/kubernetes/quick-hubble-install.yaml
#hubble_yaml=cilium-1.9.4-quick-hubble-install.yaml
kube_prometheus_download=https://github.com/prometheus-operator/kube-prometheus/archive/v0.7.0.tar.gz
kube_prometheus_dir=$proj_root/binaries/kube-prometheus
kube_proxy_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kube-proxy
kube_proxy_binary=$proj_root/binaries/kube-proxy
calico_manifest_download=https://docs.projectcalico.org/manifests/calico.yaml
calico_manifest=$proj_root/binaries/calico.yaml

#
# Verifies the existence of all the filesystem objects required to provision a cluster. If arg1 is '1' then that means
# we are creating a template VM. In that case the CentOS and VBox Guest Additions ISOs are included in the check
#
function check_files() {
  local result=0
  local objects=($containerd_binary $cni_plugins_binary $runc_binary $crictl_binary\
    $etcd_gzip $kube_apiserver_binary $kube_controller_manager_binary $kube_scheduler_binary $kubelet_binary)
  if [[ "$1" == 1 ]]; then
    objects+=($centos_iso_path)
    objects+=($guest_additions_path)
  fi
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    objects+=($metrics_server_manifest)
    objects+=($kubernetes_dashboard_manifest)
  fi
  if [[ "$monitoring" == "kube-prometheus" ]]; then
    objects+=($proj_root/binaries/kube-prometheus/setup/0namespace-namespace.yaml)
  fi
  if [[ "$networking" == "kube-router" ]]; then
    objects+=($kube_router_yaml)
  fi
  if [[ "$networking" == "calico" ]]; then
    objects+=($kube_proxy_binary)
    objects+=($calico_manifest)
  fi
#  if [[ "$networking" == "cilium" ]]; then
#    objects+=($cilium_yaml_download)
#    objects+=($hubble_yaml_download)
#  fi
  for f in "${objects[@]}"; do
    if ! [[ -f $f ]]; then
      echo "missing filesystem object: $f" >&2
      result=1
    fi
  done
  echo "$result"
}

#
# Verifies the existence of all the downloads required to provision a cluster. If arg1 is '1' then that means
# we are running from scratch. In that case the CentOS and VBox Guest Additions ISOs are included in the check
#
function check_urls() {
  local result=0
  local objects=($etcd_download $kube_apiserver_download $kube_controller_manager_download $kube_scheduler_download\
    $crictl_download $runc_download $cni_plugins_download $containerd_download $kubelet_download)
  if [[ "$1" == 1 ]]; then
    objects+=($centos_iso_download)
    objects+=($guest_additions_download)
  fi
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    objects+=($metrics_server_download)
    objects+=($kubernetes_dashboard_download)
  fi
  if [[ "$monitoring" == "kube-prometheus" ]]; then
    objects+=($kube_prometheus_download)
  fi
  if [[ "$networking" == "kube-router" ]]; then
    objects+=($kube_router_yaml_download)
  fi
  if [[ "$networking" == "calico" ]]; then
    objects+=($kube_proxy_download)
    objects+=($calico_manifest_download)
  fi
#  if [[ "$networking" == "cilium" ]]; then
#    objects+=($cilium_yaml)
#    objects+=($hubble_yaml)
#  fi
  for f in "${objects[@]}"; do
    if ! curl -sL $f -o /dev/null --head --fail; then
      echo "missing upstream resource: $f" >&2
      result=1
    fi
  done
  echo "$result"
}

function usage() {
  while IFS= read -r line; do
    echo "${line:4}"
  done <<< '
    Creates a Kubernetes cluster on the desktop using VirtualBox and CentOS kickstart for hands-free
    CentOS installation. The cluster consists of three nodes: one controller and two workers. The nodes
    are configured with VirtualBox bridge networking, which supports host-to-guest, guest-to-guest,
    and guest-to-internet.

    The script creates a "template" VM, and then clones that for each cluster node. The initial
    template VM creation runs Kickstart to install the OS, and also installs VirtualBox Guest Additions
    because that provides the ability to get the IP address from a running VM. Note - creating the template
    VM takes a fair bit of time, including the initial 60 seconds for the boot menu to time out and actually
    start the install.

    Usage: new-cluster [--help] [--host-network-interface=<interface>] [--vboxdir=</some/path>]
           [--networking=<type>] [--monitoring=<type>] [--from-scratch] [--create-template] [--single-node]
           [--up=<nodelist>] [--down=<nodelist>] [--delete=<nodelist>] [--check-compatibility]

    Options:

      --check-compatibility      Optional. If specified, checks the installed versions of various utils
                                 used by the project (curl, kubectl, etc) against what the project has been
                                 tested on - and then exits, taking no further action. You should do this
                                 at least once. Or just run "verify-prereqs" in the "scripts" directory.
      --host-network-interface   Required. The name of the primary network interface on your machine. The
                                 scripts use this to configure the VirtualBox bridge network for each node
                                 VM.
      --vboxdir                  Required. The directory where you keep VirtualBox VM files. The script uses
                                 the VBoxManage utility to create the VMs, which will in turn create a sub-
                                 directory under this directory for each VM. The directory must exist.
                                 The script will not create it.
      --networking               Optional. If specified, installs networking. Current valid values are
                                 "calico", "kube-router", and "cilium". E.g.: --networking=calico
      --from-scratch             Optional. If specified, then downloads all the necessary items - such as the k8s
                                 binaries, as well as the CentOS ISO and the Guest Additions ISO. Also creates
                                 a template (see --create-template). If not specified, then the script
                                 expects to find all required objects on the filesystem.
      --create-template          Optional. If specified, first creates a template VM to clone all the cluster
                                 nodes from before bringing up the cluster. (This step by far takes the longest.)
                                 If not specified, expects to find an existing VM to clone from. Note - if
                                 the --from-scratch option is specified, a template is always created.
      --monitoring               Optional. Installs monitoring. Allowed values are "metrics.k8s.io", and
                                 "kube-prometheus".
      --single-node              Optional. If specified, Creates a single node cluster. The default is to create
                                 one controller, named "doc", and two workers, named "ham" and "monk". This option
                                 is useful to quickly test changes since it is faster to provision a single node.
      --up                       Optional. Takes a comma-separated list of VM names, and starts them all.
      --down                     Optional. Opposite of --up. Note - this is very low-tech at present: Order
                                 the args with the workers first and the controller last.
      --delete                   Optional. Tears down the cluster: Force stops all VMs and removes associated files
                                 from the --vboxdir directory.
      --help                     Optional. Displays this help and exits.

    Examples:

      new-cluster --host-network-interface=enp0s31f6 --vboxdir=/sdb1/virtualbox --from-scratch --networking=calico

      Wires the VirtualBox bridge network to interface "enp0s31f6", uses the "/sdb1/virtualbox" directory
      for all VM files, and downloads all the upstreams during the install, placing them on the filesystem
      so they will be available for the next time. Creates a template to clone the nodes from. Installs
      cilium networking and Hubble network monitoring

      *** You must use "--from-scratch" at least once because all the upstreams are git-ignored  ***

      new-cluster --host-network-interface=enp0s31f6 --vboxdir=/sdb1/virtualbox --single-node --networking=calico

      Like the first example, except 1) gets all the upstreams from the filesystem, 2) uses the template
      VM created by a prior invocation, and 3) only creates one node.

      new-cluster --up=doc,ham,monk

      Starts the VMs for the default three-node cluster created by the script.
    '
}

#
# option parsing helper
#
function opt_val() {
  opt="$1"
  if [[ "$opt" == =* ]]; then
    echo "${opt:1}"
  else
    echo "$opt"
  fi
}

#
# parses command line parameters and sets script variables from them
#
function parse_args() {
  if [[ "$#" -eq 0 ]]; then
    usage
    exit 1
  fi
  local parsed
  parsed=$(getopt --options "" --longoptions $long_opts -n $script_name -- "$@")
  eval set -- "$parsed"
  while true; do
    case "$1" in
      --help)
        usage
        exit 0
        ;;
      --host-network-interface)
        host_network_interface=$(opt_val "$2")
        shift 2
        ;;
      --vboxdir)
        vboxdir=$(opt_val "$2")
        shift 2
        ;;
      --networking)
        networking=$(opt_val "$2")
        if [[ "$networking" != "kube-router" ]] && [[ "$networking" != "cilium" ]] && [[ "$networking" != "calico" ]]; then
          echo "unsupported parameter value for --networking option: $networking"
          exit 1
        fi
        shift 2
        ;;
      --check-compatibility)
        check_compatibility=1
        shift
        ;;
      --from-scratch)
        from_scratch=1
        downloading_msg="downloading upstreams and "
        shift
        ;;
      --create-template)
        create_template=1
        shift
        ;;
      --monitoring)
        monitoring=$(opt_val "$2")
        if [[ "$monitoring" != "metrics.k8s.io" ]] && [[ "$monitoring" != "kube-prometheus" ]]; then
          echo "unsupported parameter value for --monitoring option: $monitoring"
          exit 1
        fi
        shift 2
        ;;
      --single-node)
        single_node=1
        shift
        ;;
      --up)
        up=$(opt_val "$2")
        shift 2
        ;;
      --down)
        down=$(opt_val "$2")
        shift 2
        ;;
      --delete)
        delete=$(opt_val "$2")
        shift 2
        ;;
      --)
        shift
        break
        ;;
    esac
  done
}

# entry point

parse_args "$@"

if [[ $check_compatibility -eq 1 ]]; then
  echo "Checking version compatibility"
  $proj_root/scripts/verify-prereqs
  exit 0
fi

if [[ ! -z "$up" ]]; then
  IFS=',' read -ra vms <<< "$up"
  for vm in "${vms[@]}"; do
    VBoxManage startvm $vm
  done
  $proj_root/scripts/show-ssh $priv_key "${vms[@]}"
  exit
elif [[ ! -z "$down" ]]; then
  IFS=',' read -ra vms <<< "$down"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm"
    VBoxManage controlvm $vm acpipowerbutton
    $proj_root/scripts/wait-vm $vm --stopped
  done
  exit
elif [[ ! -z "$delete" ]]; then
  IFS=',' read -ra vms <<< "$delete"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm if running"
    VBoxManage controlvm $vm poweroff &>/dev/null && $proj_root/scripts/wait-vm $vm --stopped || echo "(not running)"
    echo "removing $vm"
    sleep 2s
    VBoxManage unregistervm $vm --delete
  done
  exit
fi

if [[ -z "$host_network_interface" ]] || [[ -z "$vboxdir" ]]; then
  echo "--host-network-interface and --vboxdir are required options"
  exit 1
fi

if [[ ! -d $vboxdir ]]; then
  echo "directory for virtualbox VMs does not exist: $vboxdir"
  exit 1
fi

if [[ $from_scratch -eq 1 ]] && ! check_urls 1; then
  exit 1
elif [[ $from_scratch -eq 0 ]] && ! check_files $create_template; then
  exit 1
fi

# create the kube-prometheus directory if it doesn't exist to simplify the .gitignore in the binaries directory
mkdir -p $proj_root/binaries/kube-prometheus

# if not installing from scratch, clear the download URLs: the scripts will use the filesystem refs instead

if [[ $from_scratch -eq 0 ]]; then
  centos_iso_download=
  guest_additions_download=
  etcd_download=
  kube_apiserver_download=
  kube_controller_manager_download=
  kube_scheduler_download=
  crictl_download=
  runc_download=
  cni_plugins_download=
  containerd_download=
  kubelet_download=
  kube_router_yaml_download=
  metrics_server_download=
  kubernetes_dashboard_download=
  kube_prometheus_download=
  kube_proxy_download=
  calico_manifest_download=
fi

# if running from scratch, always create a template VM to clone the nodes from. This step also generates
# the SSH keys into the 'kickstart' directory that are used all over the place to interact with the node VMs
# via ssh/scp as they are being bootstrapped, and that you will use to SSH into the VMs after the cluster
# is provisioned

if [[ $from_scratch -eq 1 ]] || [[ $create_template -eq 1 ]] ; then
  echo "${downloading_msg}creating a template VM"

  $proj_root/scripts/create-template-vm\
   --template-vmname=bingo\
   --centos-iso-download=$centos_iso_download\
   --centos-iso-path=$centos_iso_path\
   --guest-additions-download=$guest_additions_download\
   --guest-additions-path=$guest_additions_path\
   --host-network-interface=$host_network_interface\
   --vboxdir=$vboxdir
fi

# gen-root-ca generates $proj_root/tls/ca.pem and ca-key.pem used throughout to configure TLS

echo "generating root CA into the 'tls' directory on this machine"
$proj_root/scripts/gen-root-ca

echo "${downloading_msg}provisioning a controller node"

$proj_root/toplevel-scripts/provision-controller\
 --priv-key=$priv_key\
 --template-vmname=bingo\
 --vboxdir=$vboxdir\
 --controller-hostname=doc\
 --controller-ram=8192\
 --controller-cpu=4\
 --etcd-download=$etcd_download\
 --etcd-gzip=$etcd_gzip\
 --kube-apiserver-download=$kube_apiserver_download\
 --kube-apiserver-binary=$kube_apiserver_binary\
 --kube-controller-manager-download=$kube_controller_manager_download\
 --kube-controller-manager-binary=$kube_controller_manager_binary\
 --kube-scheduler-download=$kube_scheduler_download\
 --kube-scheduler-binary=$kube_scheduler_binary\
 --ca-cert=$proj_root/tls/ca.pem\
 --ca-key=$proj_root/tls/ca-key.pem

echo "${downloading_msg}provisioning worker nodes"

controller_ip=$($proj_root/scripts/get-vm-ip doc)

# make the controller a worker as well (use admin.kubeconfig created by provision-controller
$proj_root/scripts/configure-worker\
 --controller-ip=$controller_ip\
 --controller-hostname=doc\
 --worker-hostname=doc\
 --priv-key=$priv_key\
 --admin-kubeconfig=$admin_kubeconfig\
 --crictl-download=$crictl_download\
 --crictl-binary=$crictl_binary\
 --runc-download=$runc_download\
 --runc-binary=$runc_binary\
 --cni-plugins-download=$cni_plugins_download\
 --cni-plugins-binary=$cni_plugins_binary\
 --containerd-download=$containerd_download\
 --containerd-binary=$containerd_binary\
 --kubelet-download=$kubelet_download\
 --kubelet-binary=$kubelet_binary\
 --pod-cidr=10.200.1.0/24\
 --node-labels=controller,worker\
 --ca-cert=$proj_root/tls/ca.pem\
 --ca-key=$proj_root/tls/ca-key.pem

if [[ $single_node -eq 0 ]]; then
  cluster_vms+=(ham)
  cluster_vms+=(monk)
  $proj_root/toplevel-scripts/provision-workers\
   --worker=ham,8192,2,10.200.2.0/24\
   --worker=monk,8192,2,10.200.3.0/24\
   --controller-ip=$controller_ip\
   --controller-hostname=doc\
   --template-vmname=bingo\
   --vboxdir=$vboxdir\
   --priv-key=$priv_key\
   --crictl-download=$crictl_download\
   --crictl-binary=$crictl_binary\
   --runc-download=$runc_download\
   --runc-binary=$runc_binary\
   --cni-plugins-download=$cni_plugins_download\
   --cni-plugins-binary=$cni_plugins_binary\
   --containerd-download=$containerd_download\
   --containerd-binary=$containerd_binary\
   --kubelet-download=$kubelet_download\
   --kubelet-binary=$kubelet_binary\
   --node-labels=worker\
   --ca-cert=$proj_root/tls/ca.pem\
   --ca-key=$proj_root/tls/ca-key.pem
fi

echo "configuring /etc/hosts in VMs"
$proj_root/scripts/configure-etc-hosts $priv_key "${cluster_vms[@]}"

echo "${downloading_msg}installing pod networking"

if [[ "$networking" == "kube-router" ]]; then
  $proj_root/networking/kube-router/install-kube-router\
   --controller-ip=$controller_ip\
   --kube-router-yaml-download=$kube_router_yaml_download\
   --kube-router-yaml=$kube_router_yaml\
   --admin-kubeconfig=$admin_kubeconfig
elif [[ "$networking" == "cilium" ]]; then
  $proj_root/networking/cilium/install-cilium-networking\
   --controller-ip=$controller_ip\
   --admin-kubeconfig=$admin_kubeconfig
elif [[ "$networking" == "calico" ]]; then
  nodes="${cluster_vms[@]}"
  # convert to comma-delimited, e.g.: "doc,ham,monk":
  nodes=${nodes// /,}
  $proj_root/networking/kube-proxy/install-kube-proxy\
   --controller-ip=$controller_ip\
   --kube-proxy-download=$kube_proxy_download\
   --kube-proxy-binary=$kube_proxy_binary\
   --priv-key=$priv_key\
   --nodes=$nodes\
   --ca-cert=$proj_root/tls/ca.pem\
   --ca-key=$proj_root/tls/ca-key.pem
  $proj_root/networking/calico/install-calico-networking\
   --calico-manifest-download=$calico_manifest_download\
   --calico-manifest=$calico_manifest\
   --priv-key=$priv_key\
   --admin-kubeconfig=$admin_kubeconfig\
   --pod-cidr=10.200.0.0/16\
   --nodes=$nodes
fi

echo "${downloading_msg}installing cluster DNS"

$proj_root/dns/coredns/install-coredns\
 --replicas=${#workers[@]}\
 --admin-kubeconfig=$admin_kubeconfig

if [[ -n "$monitoring" ]]; then
  echo "${downloading_msg}installing Monitoring"
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    $proj_root/monitoring/metrics-server/install-metrics-server\
     --priv-key=$priv_key\
     --controller-ip=$controller_ip\
     --admin-kubeconfig=$admin_kubeconfig\
     --metrics-server-download=$metrics_server_download\
     --metrics-server-manifest=$metrics_server_manifest\
     --kubernetes-dashboard-download=$kubernetes_dashboard_download\
     --kubernetes-dashboard-manifest=$kubernetes_dashboard_manifest\
     --ca-cert=$proj_root/tls/ca.pem\
     --ca-key=$proj_root/tls/ca-key.pem
  elif [[ "$monitoring" == "kube-prometheus" ]]; then
    $proj_root/monitoring/kube-prometheus/install-kube-prometheus\
     --controller-ip=$controller_ip\
     --kube-prometheus-download=$kube_prometheus_download\
     --kube-prometheus-dir=$kube_prometheus_dir\
     --admin-kubeconfig=$proj_root/admin/admin.kubeconfig
  fi
fi

echo
echo "finished provisioning cluster. To interact with the cluster:"
echo "  export KUBECONFIG=$admin_kubeconfig"
echo

$proj_root/scripts/show-ssh $priv_key "${cluster_vms[@]}"
