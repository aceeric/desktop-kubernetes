#!/usr/bin/env bash

set -e

long_opts=help,host-network-interface:,vboxdir:,check-compatibility,from-scratch,create-template,up:,down:,delete:
script_name=$(basename "$0")
proj_root="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
priv_key=$proj_root/kickstart/id_ed25519
admin_kubeconfig=$proj_root/admin/admin.kubeconfig

# api
host_network_interface=
vboxdir=
from_scratch=0
create_template=0
check_compatibility=0
up=
down=
delete=

# internal
downloading_msg=""

# These are the various binaries and manifests that are downloaded by the scripts in the project. Two
# elements are defined for each upstream: the URL to get it from, and the filesystem location. This way,
# once the script is run "from scratch", subsequent invocations can just use the downloaded objects. ISOs
# go in the 'media' directory, and everything else goes in the 'binaries' directory. The kickstart ISO
# is not downloaded, it is generated by the project. All these objects are git-ignored

centos_iso_download=http://mirror.umd.edu/centos/8.3.2011/isos/x86_64/CentOS-8.3.2011-x86_64-dvd1.iso
centos_iso_path=$proj_root/media/CentOS-8.3.2011-x86_64-dvd1.iso
guest_additions_download=http://download.virtualbox.org/virtualbox/6.1.18/VBoxGuestAdditions_6.1.18.iso
guest_additions_path=$proj_root/media/VBoxGuestAdditions_6.1.18.iso
etcd_download=https://github.com/etcd-io/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz
etcd_gzip=$proj_root/binaries/etcd-v3.4.14-linux-amd64.tar.gz
kube_apiserver_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kube-apiserver
kube_apiserver_binary=$proj_root/binaries/kube-apiserver
kube_controller_manager_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kube-controller-manager
kube_controller_manager_binary=$proj_root/binaries/kube-controller-manager
kube_scheduler_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kube-scheduler
kube_scheduler_binary=$proj_root/binaries/kube-scheduler
crictl_download=https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.19.0/crictl-v1.19.0-linux-amd64.tar.gz
crictl_binary=$proj_root/binaries/crictl-v1.19.0-linux-amd64.tar.gz
runc_download=https://github.com/opencontainers/runc/releases/download/v1.0.0-rc92/runc.amd64
runc_binary=$proj_root/binaries/runc
cni_plugins_download=https://github.com/containernetworking/plugins/releases/download/v0.9.0/cni-plugins-linux-amd64-v0.9.0.tgz
cni_plugins_binary=$proj_root/binaries/cni-plugins-linux-amd64-v0.9.0.tgz
containerd_download=https://github.com/containerd/containerd/releases/download/v1.4.3/containerd-1.4.3-linux-amd64.tar.gz
containerd_binary=$proj_root/binaries/containerd-1.4.3-linux-amd64.tar.gz
kubelet_download=https://storage.googleapis.com/kubernetes-release/release/v1.20.1/bin/linux/amd64/kubelet
kubelet_binary=$proj_root/binaries/kubelet
kube_router_yaml_download=https://raw.githubusercontent.com/cloudnativelabs/kube-router/v1.1.1/daemonset/generic-kuberouter-all-features.yaml
kube_router_yaml=$proj_root/binaries/generic-kuberouter-all-features.yaml

#
# Verifies the existence of all the filesystem objects required to provision a cluster. If arg1 is '1' then that means
# we are creating a template VM. In that case the CentOS and VBox Guest Additions ISOs are included in the check
#
function check_files() {
  local result=0
  local objects=($kube_router_yaml $containerd_binary $cni_plugins_binary $runc_binary $crictl_binary\
    $etcd_gzip $kube_apiserver_binary $kube_controller_manager_binary $kube_scheduler_binary $kubelet_binary)
  if [[ "$1" == 1 ]]; then
    objects+=($centos_iso_path)
    objects+=($guest_additions_path)
  fi
  for f in "${objects[@]}"; do
    if ! [[ -f $f ]]; then
      echo "missing filesystem object: $f" >&2
      result=1
    fi
  done
  echo "$result"
}

#
# Verifies the existence of all the downloads required to provision a cluster. If arg1 is '1' then that means
# we are running from scratch. In that case the CentOS and VBox Guest Additions ISOs are included in the check
#
function check_urls() {
  local result=0
  local objects=($etcd_download $kube_apiserver_download $kube_controller_manager_download $kube_scheduler_download\
    $crictl_download $runc_download $cni_plugins_download $containerd_download $kubelet_download\
    $kube_router_yaml_download)
  if [[ "$1" == 1 ]]; then
    objects+=($centos_iso_download)
    objects+=($guest_additions_download)
  fi
  for f in "${objects[@]}"; do
    if ! curl -sL $f -o /dev/null --head --fail; then
      echo "missing upstream resource: $f" >&2
      result=1
    fi
  done
  echo "$result"
}

function usage() {
  while IFS= read -r line; do
    echo "${line:4}"
  done <<< '
    Creates a Kubernetes cluster on the desktop using VirtualBox and CentOS kickstart for hands-free
    CentOS installation. The cluster consists of three nodes: one controller and two workers. The nodes
    are configured with VirtualBox bridge networking, which supports host-to-guest, guest-to-guest,
    and guest-to-internet.

    The script creates a "template" VM, and then clones that for each cluster node. The initial
    template VM creation runs Kickstart to install the OS, and also installs VirtualBox Guest Additions
    because that provides the ability to get the IP address from a running VM. Note - creating the template
    VM takes a fair bit of time, including the initial 60 seconds for the boot menu to time out and actually
    start the install.

    Usage: new-cluster [--host-network-interface=<interface>] [--vboxdir=</some/path>]
           [--check-compatibility] [--from-scratch] [--create-template]
           [--help] [--up=<nodelist>] [--down=<nodelist>] [--delete=<nodelist>]


    Options:

      --host-network-interface   Required. The name of the primary network interface on your machine. The
                                 scripts use this to configure the VirtualBox bridge network for each node
                                 VM.
      --vboxdir                  Required. The directory where you keep VirtualBox VM files. The script uses
                                 the VBoxManage utility to create the VMs, which will in turn create a sub-
                                 directory under this directory for each VM. The directory must exist.
                                 The script will not create it.
      --check-compatibility      Optional. If specified, checks the installed versions of various utils
                                 used by the project (curl, kubectl, etc) against what the project has been
                                 tested on - and then exits, taking no further action. You should do this
                                 at least once. Or just run "verify-prereqs" in the "scripts" directory.
      --from-scratch             Optional. If specified, then downloads all the necessary items - such as the k8s
                                 binaries, as well as the CentOS ISO and the Guest Additions ISO. Also creates
                                 a template (see --create-template). If not specified, then the script
                                 expects to find all required objects on the filesystem.
      --create-template          Optional. If specified, first creates a template VM to clone all the cluster
                                 nodes from before bringing up the cluster. (This step by far takes the longest.)
                                 If not specified, expects to find an existing VM to clone from. Note - if
                                 the --from-scratch option is specified, a template is always created.
      --up                       Optional. Takes a comma-separated list of VM names, and starts them all.
      --down                     Optional. Opposite of --up. Note - this is very low-tech at present: Order
                                 the args with the workers first and the controller last.
      --delete                   Optional. Removes VMs along with associated files from the --vboxdir directory.
      --help                     Optional. Displays this help and exits.
    Examples:

      new-cluster --host-network-interface=enp0s31f6 --vboxdir=/sdb1/virtualbox --from-scratch

      Wires the VirtualBox bridge network to interface "enp0s31f6", uses the "/sdb1/virtualbox" directory
      for all VM files, and downloads all the upstreams during the install, placing them on the filesystem
      so they will be available for the next time. Creates a template to clone the nodes from.

      *** You must use "--from-scratch" at least once because all the upstreams are git-ignored  ***

      new-cluster --host-network-interface=enp0s31f6 --vboxdir=/sdb1/virtualbox

      Same as the first example, except 1) gets all the upstreams from the filesystem, and 2) uses the
      template VM created by the prior invocation.

      new-cluster --up=doc,ham,monk

      Starts the VMs for the default three-node cluster created by the script.
    '
}

function opt_val() {
  opt="$1"
  if [[ "$opt" == =* ]]; then
    echo "${opt:1}"
  else
    echo "$opt"
  fi
}

function parse_args() {
  if [[ "$#" -eq 0 ]]; then
    usage
    exit 1
  fi
  local parsed
  parsed=$(getopt --options "" --longoptions $long_opts -n $script_name -- "$@")
  eval set -- "$parsed"
  while true; do
    case "$1" in
      --help)
        usage
        exit 0
        ;;
      --host-network-interface)
        host_network_interface=$(opt_val "$2")
        shift 2
        ;;
      --vboxdir)
        vboxdir=$(opt_val "$2")
        shift 2
        ;;
      --check-compatibility)
        check_compatibility=1
        shift
        ;;
      --from-scratch)
        from_scratch=1
        downloading_msg="downloading upstreams and "
        shift
        ;;
      --create-template)
        create_template=1
        shift
        ;;
      --up)
        up=$(opt_val "$2")
        shift 2
        ;;
      --down)
        down=$(opt_val "$2")
        shift 2
        ;;
      --delete)
        delete=$(opt_val "$2")
        shift 2
        ;;
      --)
        shift
        break
        ;;
    esac
  done
}

# entry point - parses args and performs some basic validations

parse_args "$@"

if [[ $check_compatibility -eq 1 ]]; then
  echo "Checking version compatibility"
  $proj_root/scripts/verify-prereqs
  exit 0
fi

if [[ ! -z "$up" ]]; then
  IFS=',' read -ra vms <<< "$up"
  for vm in "${vms[@]}"; do
    VBoxManage startvm $vm
  done
  $proj_root/scripts/show-ssh $priv_key
  exit
elif [[ ! -z "$down" ]]; then
  IFS=',' read -ra vms <<< "$down"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm"
    VBoxManage controlvm $vm acpipowerbutton
    $proj_root/scripts/wait-vm $vm --stopped
  done
  exit
elif [[ ! -z "$delete" ]]; then
  IFS=',' read -ra vms <<< "$delete"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm if running"
    VBoxManage controlvm $vm poweroff &>/dev/null && $proj_root/scripts/wait-vm $vm --stopped || echo "(not running)"
    echo "removing $vm"
    sleep 2s
    VBoxManage unregistervm $vm --delete
  done
  exit
fi

if [[ -z "$host_network_interface" ]] || [[ -z "$vboxdir" ]]; then
  echo "--host-network-interface and --vboxdir are required options"
  exit 1
fi

if [[ ! -d $vboxdir ]]; then
  echo "directory for virtualbox VMs does not exist: $vboxdir"
  exit 1
fi

if [[ $from_scratch -eq 1 ]] && ! check_urls 1; then
  exit 1
elif [[ $from_scratch -eq 0 ]] && ! check_files $create_template; then
  exit 1
fi

# if not installing from scratch, clear the download URLs: the scripts will use the filesystem refs instead

if [[ $from_scratch -eq 0 ]]; then
  centos_iso_download=
  guest_additions_download=
  etcd_download=
  kube_apiserver_download=
  kube_controller_manager_download=
  kube_scheduler_download=
  crictl_download=
  runc_download=
  cni_plugins_download=
  containerd_download=
  kubelet_download=
  kube_router_yaml_download=
fi

# if running from scratch, always create a template VM to clone the nodes from. This step also generates
# the SSH keys into the 'kickstart' directory that are used all over the place to interact with the node VMs
# via ssh/scp as they are being bootstrapped, and that you will use to SSH into the VMs after the cluster
# is provisioned

if [[ $from_scratch -eq 1 ]] || [[ $create_template -eq 1 ]] ; then
  echo "${downloading_msg}creating a template VM"

  $proj_root/scripts/create-template-vm\
   --template-vmname=bingo\
   --centos-iso-download=$centos_iso_download\
   --centos-iso-path=$centos_iso_path\
   --guest-additions-download=$guest_additions_download\
   --guest-additions-path=$guest_additions_path\
   --host-network-interface=$host_network_interface\
   --vboxdir=$vboxdir
fi

# gen-root-ca generates $proj_root/tls/ca.pem and ca-key.pem used throughout to configure TLS

echo "generating root CA into the 'tls' directory on this machine"
$proj_root/scripts/gen-root-ca

echo "${downloading_msg}provisioning a controller node"

$proj_root/toplevel-scripts/provision-controller\
 --priv-key=$priv_key\
 --template-vmname=bingo\
 --vboxdir=$vboxdir\
 --controller-hostname=doc\
 --controller-ram=4096\
 --controller-cpu=2\
 --etcd-download=$etcd_download\
 --etcd-gzip=$etcd_gzip\
 --kube-apiserver-download=$kube_apiserver_download\
 --kube-apiserver-binary=$kube_apiserver_binary\
 --kube-controller-manager-download=$kube_controller_manager_download\
 --kube-controller-manager-binary=$kube_controller_manager_binary\
 --kube-scheduler-download=$kube_scheduler_download\
 --kube-scheduler-binary=$kube_scheduler_binary\
 --ca-cert=$proj_root/tls/ca.pem\
 --ca-key=$proj_root/tls/ca-key.pem

echo "${downloading_msg}provisioning worker nodes"

controller_ip=$($proj_root/scripts/get-vm-ip doc)

# make the controller a worker as well (use admin.kubeconfig created by provision-controller
$proj_root/scripts/configure-worker\
 --controller-ip=$controller_ip\
 --controller-hostname=doc\
 --worker-hostname=doc\
 --priv-key=$priv_key\
 --admin-kubeconfig=$admin_kubeconfig\
 --crictl-download=$crictl_download\
 --crictl-binary=$crictl_binary\
 --runc-download=$runc_download\
 --runc-binary=$runc_binary\
 --cni-plugins-download=$cni_plugins_download\
 --cni-plugins-binary=$cni_plugins_binary\
 --containerd-download=$containerd_download\
 --containerd-binary=$containerd_binary\
 --kubelet-download=$kubelet_download\
 --kubelet-binary=$kubelet_binary\
 --pod-cidr=10.200.1.0/24\
 --node-labels=controller,worker\
 --ca-cert=$proj_root/tls/ca.pem\
 --ca-key=$proj_root/tls/ca-key.pem

$proj_root/toplevel-scripts/provision-workers\
 --worker=ham,8192,2,10.200.2.0/24\
 --worker=monk,8192,2,10.200.3.0/24\
 --controller-ip=$controller_ip\
 --controller-hostname=doc\
 --template-vmname=bingo\
 --vboxdir=$vboxdir\
 --priv-key=$priv_key\
 --crictl-download=$crictl_download\
 --crictl-binary=$crictl_binary\
 --runc-download=$runc_download\
 --runc-binary=$runc_binary\
 --cni-plugins-download=$cni_plugins_download\
 --cni-plugins-binary=$cni_plugins_binary\
 --containerd-download=$containerd_download\
 --containerd-binary=$containerd_binary\
 --kubelet-download=$kubelet_download\
 --kubelet-binary=$kubelet_binary\
 --node-labels=worker\
 --ca-cert=$proj_root/tls/ca.pem\
 --ca-key=$proj_root/tls/ca-key.pem

echo "configuring /etc/hosts in VMs"
$proj_root/scripts/configure-etc-hosts $priv_key doc ham monk

echo "${downloading_msg}installing pod networking"


$proj_root/worker/kube-router/install-kube-router\
 --controller-ip=$controller_ip\
 --kube-router-yaml-download=$kube_router_yaml_download\
 --kube-router-yaml=$kube_router_yaml\
 --admin-kubeconfig=$admin_kubeconfig

# Wait for kube-router to become fully ready. Otherwise when we install CoreDNS, the CoreDNS pod comes
# up with an invalid IP address (10.88.something) and then it doesn't function correctly. Unclear presently
# how it actually comes up with that IP because 88 is specified nowhere that I can find

echo "${downloading_msg}waiting for pod networking"

ready=0
desired=1
while [[ $ready -ne $desired ]]; do
  sleep 1s
  ready=$(kubectl --kubeconfig $admin_kubeconfig -n kube-system get ds kube-router -ojsonpath='{.status.numberReady}')
  desired=$(kubectl --kubeconfig $admin_kubeconfig -n kube-system get ds kube-router -ojsonpath='{.status.desiredNumberScheduled}')
done

echo "${downloading_msg}installing cluster DNS"

$proj_root/dns/coredns/install-coredns\
 --controller-ip=$controller_ip\
 --replicas=${#workers[@]}\
 --admin-kubeconfig=$admin_kubeconfig

echo
echo "finished provisioning workers. To interact with the cluster:"
echo "    export KUBECONFIG=$admin_kubeconfig"
echo

$proj_root/scripts/show-ssh $priv_key
