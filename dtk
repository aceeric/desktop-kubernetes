#!/usr/bin/env bash

set -e

long_opts=help,host-network-interface:,host-only-network:,vboxdir:,check-compatibility,create-template,networking:,\
monitoring:,storage:,single-node,up:,down:,delete:,verify:,sshto:,template-name:,linux:,containerized-cplane:

# api
host_network_interface=
host_only_network=
vboxdir=
check_compatibility=0
create_template=0
networking=
monitoring=
storage=
single_node=0
up=
down=
delete=
verify=
sshto=
template_name=bingo
linux=centos
containerized_cplane=

# internal
cluster_vms=(doc)

# This snippet enables all scripts to exec all other scripts without knowing any
# other script's path, as long all the scripts (except this one) are children
# of the 'scripts' directory. In the case where you're directly testing child scripts,
# you must define DTKBASE and the xec function in your console first.
export DTKBASE="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
function xec() { f=$(find $DTKBASE/scripts -name $1) && $f "${@:2}"; }
export -f xec

source $DTKBASE/artifacts

#
# Verifies the existence of all the filesystem objects required to provision a cluster. If arg1 is '1' then that means
# we are creating a template VM. In that case the OS and VBox Guest Additions ISOs are included in the check
#
function check_files() {
  local result=0
  local objects=($containerd_binary $cni_plugins_binary $runc_binary $crictl_binary\
    $etcd_gzip $kube_apiserver_binary $kube_controller_manager_binary $kube_scheduler_binary $kubelet_binary,$kubernetes_dashboard_manifest)
  if [[ "$1" == 1 ]]; then
    objects+=($guest_additions_path)
    if [[ $linux == "centos" ]]; then
      objects+=($centos_iso_path)
    else
      objects+=($rocky_iso_path)
    fi
  fi
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    objects+=($metrics_server_manifest)
  fi
  if [[ "$monitoring" == "kube-prometheus" ]]; then
    objects+=($kube_prometheus_binary)
  fi
  if [[ "$networking" == "kube-router" ]]; then
    objects+=($kube_router_yaml)
  fi
  if [[ "$networking" == "calico" ]]; then
    objects+=($kube_proxy_binary)
    objects+=($calico_tigera_manifest)
    objects+=($calico_custom_resources_manifest)
  fi
  if [[ "$storage" == "openebs" ]]; then
    objects+=($openebs_hostpath_operator_manifest)
    objects+=($openebs_sc_manifest)
  fi
#  if [[ "$networking" == "cilium" ]]; then
#    objects+=($cilium_yaml_download)
#    objects+=($hubble_yaml_download)
#  fi
  for f in "${objects[@]}"; do
    if ! [[ -f $f ]]; then
      echo "missing filesystem object: $f" >&2
      result=1
    else
      echo "OK: $f"
    fi
  done
  echo "$result"
}

#
# Verifies the existence of all the downloads required to provision a cluster. If arg1 is '1' then
# the OS and VBox Guest Additions ISOs are included in the check
#
function check_urls() {
  local result=0
  local objects=($etcd_download $kube_apiserver_download $kube_controller_manager_download $kube_scheduler_download\
    $crictl_download $runc_download $cni_plugins_download $containerd_download $kubelet_download, $kubernetes_dashboard_download)
  if [[ "$1" == 1 ]]; then
    objects+=($guest_additions_download)
    if [[ $linux == "centos" ]]; then
      objects+=($centos_iso_download)
    else
      objects+=($rocky_iso_download)
    fi
  fi
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    objects+=($metrics_server_download)
  fi
  if [[ "$monitoring" == "kube-prometheus" ]]; then
    objects+=($kube_prometheus_download)
  fi
  if [[ "$networking" == "kube-router" ]]; then
    objects+=($kube_router_yaml_download)
  fi
  if [[ "$networking" == "calico" ]]; then
    objects+=($kube_proxy_download)
    objects+=($calico_tigera_download)
    objects+=($calico_custom_resources_download)
  fi
  if [[ "$storage" == "openebs" ]]; then
    objects+=($openebs_hostpath_operator_manifest_download)
    objects+=($openebs_sc_manifest_download)
  fi
#  if [[ "$networking" == "cilium" ]]; then
#    objects+=($cilium_yaml)
#    objects+=($hubble_yaml)
#  fi
  for f in "${objects[@]}"; do
    if ! curl -sL $f -o /dev/null --head --fail; then
      echo "missing upstream resource: $f" >&2
      result=1
    else
      echo "OK: $f"
    fi
  done
  echo "$result"
}

#
# option parsing helper
#
function opt_val() {
  opt="$1"
  if [[ "$opt" == =* ]]; then
    echo "${opt:1}"
  else
    echo "$opt"
  fi
}

#
# parses command line parameters and sets script variables from them
#
function parse_args() {
  if [[ "$#" -eq 0 ]]; then
    xec show-usage
    exit 1
  fi
  local parsed
  local script_name=$(basename "$0")
  parsed=$(getopt --options "" --longoptions $long_opts -n $script_name -- "$@")
  eval set -- "$parsed"
  while true; do
    case "$1" in
      --help)
        xec show-usage
        exit 0
        ;;
      --host-network-interface)
        host_network_interface=$(opt_val "$2")
        shift 2
        ;;
      --host-only-network)
        host_only_network=$(opt_val "$2")
        shift 2
        ;;
      --vboxdir)
        vboxdir=$(opt_val "$2")
        shift 2
        ;;
      --networking)
        networking=$(opt_val "$2")
        if [[ "$networking" != "kube-router" ]] && [[ "$networking" != "cilium" ]] && [[ "$networking" != "calico" ]]; then
          echo "unsupported parameter value for --networking option: $networking"
          exit 1
        fi
        shift 2
        ;;
      --check-compatibility)
        check_compatibility=1
        shift
        ;;
      --create-template)
        create_template=1
        shift
        ;;
      --monitoring)
        monitoring=$(opt_val "$2")
        if [[ "$monitoring" != "metrics.k8s.io" ]] && [[ "$monitoring" != "kube-prometheus" ]]; then
          echo "unsupported parameter value for --monitoring option: $monitoring"
          exit 1
        fi
        shift 2
        ;;
      --storage)
        storage=$(opt_val "$2")
        if [[ "$storage" != "openebs" ]]; then
          echo "unsupported parameter value for --storage option: $storage"
          exit 1
        fi
        shift 2
        ;;
      --containerized-cplane)
        containerized_cplane=$(opt_val "$2")
        shift 2
        ;;
      --single-node)
        single_node=1
        shift
        ;;
      --up)
        up=$(opt_val "$2")
        shift 2
        ;;
      --down)
        down=$(opt_val "$2")
        shift 2
        ;;
      --delete)
        delete=$(opt_val "$2")
        shift 2
        ;;
      --verify)
        verify=$(opt_val "$2")
        if [[ "$verify" != "upstreams" ]] && [[ "$verify" != "files" ]]; then
          echo "unsupported parameter value for --verify option: $verify"
          exit 1
        fi
        shift 2
        ;;
      --sshto)
        sshto=$(opt_val "$2")
        shift 2
        ;;
      --template-name)
        template_name=$(opt_val "$2")
        shift 2
        ;;
      --linux)
        linux=$(opt_val "$2")
        if [[ "$linux" != "centos" ]] && [[ "$linux" != "rocky" ]]; then
          echo "unsupported parameter value for --linux option: $linux"
          exit 1
        fi
        shift 2
        ;;
      --)
        shift
        break
        ;;
    esac
  done

  if [[ $# -ne 0 ]]; then
    echo "Unsupported command line option(s): $@"
    exit 1
  fi
}

# entry point

parse_args "$@"

if [[ $check_compatibility -eq 1 ]]; then
  echo "Checking version compatibility"
  xec verify-prereqs
  exit 0
elif [[ "$verify" == "upstreams" ]]; then
  check_urls $create_template
  exit 0
elif [[ "$verify" == "files" ]]; then
  check_files $create_template
  exit 0
elif [[ ! -z "$up" ]]; then
  IFS=',' read -ra vms <<< "$up"
  for vm in "${vms[@]}"; do
    VBoxManage startvm $vm
  done
  xec show-ssh $DTKBASE/generated/kickstart/id_ed25519 "${vms[@]}"
  exit 0
elif [[ ! -z "$down" ]]; then
  IFS=',' read -ra vms <<< "$down"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm"
    VBoxManage controlvm $vm acpipowerbutton
    xec wait-vm $vm --stopped
  done
  exit 0
elif [[ ! -z "$delete" ]]; then
  IFS=',' read -ra vms <<< "$delete"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm if running"
    VBoxManage controlvm $vm poweroff &>/dev/null && xec wait-vm $vm --stopped || echo "(not running)"
    echo "removing $vm"
    sleep 2s
    VBoxManage unregistervm $vm --delete
  done
  exit 0
elif [[ ! -z "$sshto" ]]; then
  ip=$(xec get-vm-ip $sshto)
  ssh -i $DTKBASE/generated/kickstart/id_ed25519 root@$ip
fi

if [[ -z "$host_network_interface" ]] && [[ -z "$host_only_network" ]]; then
  echo "either --host-network-interface or --host-only-network is required"
  exit 1
elif [[ ! -z "$host_network_interface" ]] && [[ ! -z "$host_only_network" ]]; then
  echo "--host-network-interface and --host-only-network are exclusive of each other"
  exit 1
elif [[ -z "$vboxdir" ]]; then
  echo "--vboxdir is a required option"
  exit 1
elif [[ ! -d $vboxdir ]]; then
  echo "directory for virtualbox VMs does not exist: $vboxdir"
  exit 1
fi

# create directories to generate various files into
mkdir -p $DTKBASE/generated/kickstart $DTKBASE/generated/kubeconfig\
         $DTKBASE/generated/cert $DTKBASE/generated/hostonly-netcfg $DTKBASE/generated/iso

# Create a template VM to clone the nodes from. This step also generates the SSH keys into the 'generated/kickstart'
# directory that are used all over the place to interact with the node VMs via ssh/scp as they are being bootstrapped,
# and that you will use to SSH into the VMs after the cluster is provisioned. If using host only networking, the
# 'create-template-vm' script will also create a host only VirtualBox network which will be used to initialize the
# template. This host only network name will be copied into every VM cloned from the template. So every cloned VM
# will be on the same host only network.
#
# Script args are validated above so caller never specifies both bridge (by virtue of the --host-network-interface
# option) AND host only (by virtue of the --host-only-network option)

if [[ $create_template -eq 1 ]] ; then
  echo "creating a template VM"

  if [[ $linux == "centos" ]]; then
    linux_iso_download=$centos_iso_download
    linux_iso_path=$centos_iso_path
  else
    linux_iso_download=$rocky_iso_download
    linux_iso_path=$rocky_iso_path
  fi
  xec create-template-vm\
   --template-vmname=$template_name\
   --linux-iso-download=$linux_iso_download\
   --linux-iso-path=$linux_iso_path\
   --guest-additions-download=$guest_additions_download\
   --guest-additions-path=$guest_additions_path\
   --host-network-interface=$host_network_interface\
   --host-only-network=$host_only_network\
   --vboxdir=$vboxdir
fi

# gen-root-ca generates $DTKBASE/generated/cert/ca.pem and ca-key.pem used throughout to configure TLS

echo "generating root CA"
xec gen-root-ca

echo "Generating Kubernetes core cluster"
xec gen-core-k8s\
 --host-only-network=$host_only_network\
 --vboxdir=$vboxdir\
 --single-node=$single_node\
 --template-vmname=$template_name\
 --containerized-cplane=$containerized_cplane\
 --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
 --ca-cert=$DTKBASE/generated/cert/ca.pem\
 --ca-key=$DTKBASE/generated/cert/ca-key.pem

controller_ip=$(xec get-vm-ip doc)

# todo these names are also specified in gen-core-k8s
if [[ $single_node -eq 0 ]]; then
  cluster_vms+=(ham)
  cluster_vms+=(monk)
fi

echo "installing pod networking"

if [[ "$networking" == "kube-router" ]]; then
  xec install-kube-router\
   --controller-ip=$controller_ip\
   --kube-router-yaml-download=$kube_router_yaml_download\
   --kube-router-yaml=$kube_router_yaml\
   --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
elif [[ "$networking" == "cilium" ]]; then
  xec install-cilium-networking\
   --controller-ip=$controller_ip\
   --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
elif [[ "$networking" == "calico" ]]; then
  nodes="${cluster_vms[@]}"
  # convert array (x y z) to comma-delimited string "x,y,z":
  nodes=${nodes// /,}
  xec install-kube-proxy\
   --controller-ip=$controller_ip\
   --kube-proxy-download=$kube_proxy_download\
   --kube-proxy-binary=$kube_proxy_binary\
   --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
   --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
   --containerized-cplane=$containerized_cplane\
   --ca-cert=$DTKBASE/generated/cert/ca.pem\
   --ca-key=$DTKBASE/generated/cert/ca-key.pem\
   --nodes=$nodes
  xec install-calico-networking\
   --calico-tigera-download=$calico_tigera_download\
   --calico-tigera-binary=$calico_tigera_manifest\
   --calico-custom-resources-download=$calico_custom_resources_download\
   --calico-custom-resources-manifest=$calico_custom_resources_manifest\
   --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
   --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
   --cluster-cidr=10.200.0.0/16\
   --nodes=$nodes
fi

echo "installing cluster DNS"

xec install-coredns\
 --replicas=${#workers[@]}\
 --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig

if [[ -n "$monitoring" ]]; then
  echo "installing Monitoring"
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    xec install-metrics-server\
     --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
     --metrics-server-download=$metrics_server_download\
     --metrics-server-manifest=$metrics_server_manifest
  elif [[ "$monitoring" == "kube-prometheus" ]]; then
    nodes="${cluster_vms[@]}"
    nodes=${nodes// /,}
    xec install-kube-prometheus\
     --nodes=$nodes\
     --kube-prometheus-download=$kube_prometheus_download\
     --kube-prometheus-binary=$kube_prometheus_binary\
     --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
  fi
fi

if [[ -n "$storage" ]]; then
  echo "installing a storage provisioner"
  if [[ "$storage" == "openebs" ]]; then
    xec install-openebs\
     --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
     --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
     --openebs-hostpath-operator-manifest-download=$openebs_hostpath_operator_manifest_download\
     --openebs-hostpath-operator-manifest=$openebs_hostpath_operator_manifest\
     --openebs-sc-manifest-download=$openebs_sc_manifest_download\
     --openebs-sc-manifest=$openebs_sc_manifest
  fi
fi

echo "installing Kubernetes Dashboard"

xec install-kubernetes-dashboard\
 --controller-ip=$controller_ip\
 --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
 --kubernetes-dashboard-download=$kubernetes_dashboard_download\
 --kubernetes-dashboard-manifest=$kubernetes_dashboard_manifest

echo
echo "finished provisioning cluster. To interact with the cluster:"
echo "  export KUBECONFIG=$DTKBASE/generated/kubeconfig/admin.kubeconfig"
echo

xec show-ssh $DTKBASE/generated/kickstart/id_ed25519 "${cluster_vms[@]}"
