#!/usr/bin/env bash

set -e

long_opts=help,host-network-interface:,host-only-network:,vboxdir:,check-compatibility,create-template,networking:,\
monitoring:,storage:,single-node,up:,down:,delete:,verify:,sshto:,template-name:,linux:

# api
host_network_interface=
host_only_network=
vboxdir=
check_compatibility=0
create_template=0
networking=
monitoring=
storage=
single_node=0
up=
down=
delete=
verify=
sshto=
template_name=bingo
linux=centos

# internal
cluster_vms=(doc)
controller_octet=
worker1_octet=
worker2_octet=

# This snippet enables all scripts to exec all other scripts without knowing any
# other script's path, as long all the scripts (except this one) are children
# of the 'scripts' directory. In the case where you're directly testing child scripts,
# you must define DTKBASE and the xec function in your console first.
export DTKBASE="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
function xec() { f=$(find $DTKBASE/scripts -name $1) && $f "${@:2}"; }
export -f xec

# These are the various binaries and manifests that are downloaded by the scripts in the project. Two
# elements are defined for each upstream: the URL to get it from, and the filesystem location. This way,
# once the script is run once, subsequent invocations can just use the downloaded objects. All downloads
# are placed into in the 'binaries' directory.

# core
centos_iso_download=https://mirror.umd.edu/centos/8-stream/isos/x86_64/CentOS-Stream-8-x86_64-latest-dvd1.iso
centos_iso_path=$DTKBASE/binaries/CentOS-Stream-8-x86_64-latest-dvd1.iso
rocky_iso_download=https://download.rockylinux.org/pub/rocky/9/isos/x86_64/Rocky-9.0-x86_64-dvd.iso
rocky_iso_path=$DTKBASE/binaries/Rocky-9.0-x86_64-dvd.iso
guest_additions_download=https://download.virtualbox.org/virtualbox/6.1.36/VBoxGuestAdditions_6.1.36.iso
guest_additions_path=$DTKBASE/binaries/VBoxGuestAdditions_6.1.36.iso
etcd_download=https://github.com/etcd-io/etcd/releases/download/v3.5.4/etcd-v3.5.4-linux-amd64.tar.gz
etcd_gzip=$DTKBASE/binaries/etcd-v3.5.4-linux-amd64.tar.gz
kube_apiserver_download=https://dl.k8s.io/v1.25.0/bin/linux/amd64/kube-apiserver
kube_apiserver_binary=$DTKBASE/binaries/kube-apiserver-v1.25.0
kube_controller_manager_download=https://dl.k8s.io/v1.25.0/bin/linux/amd64/kube-controller-manager
kube_controller_manager_binary=$DTKBASE/binaries/kube-controller-manager-v1.25.0
kube_scheduler_download=https://dl.k8s.io/v1.25.0/bin/linux/amd64/kube-scheduler
kube_scheduler_binary=$DTKBASE/binaries/kube-scheduler-v1.25.0
crictl_download=https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.2/crictl-v1.24.2-linux-amd64.tar.gz
crictl_binary=$DTKBASE/binaries/crictl-v1.24.2-linux-amd64.tar.gz
runc_download=https://github.com/opencontainers/runc/releases/download/v1.1.1/runc.amd64
runc_binary=$DTKBASE/binaries/runc
cni_plugins_download=https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz
cni_plugins_binary=$DTKBASE/binaries/cni-plugins-linux-amd64-v1.1.1.tgz
containerd_download=https://github.com/containerd/containerd/releases/download/v1.6.4/containerd-1.6.4-linux-amd64.tar.gz
containerd_binary=$DTKBASE/binaries/containerd-1.6.4-linux-amd64.tar.gz
kubelet_download=https://dl.k8s.io/v1.25.0/bin/linux/amd64/kubelet
kubelet_binary=$DTKBASE/binaries/kubelet-v1.25.0
kube_router_yaml_download=https://raw.githubusercontent.com/cloudnativelabs/kube-router/v1.3.1/daemonset/generic-kuberouter-all-features.yaml
kube_router_yaml=$DTKBASE/binaries/generic-kuberouter-all-features.yaml
kube_proxy_download=https://dl.k8s.io/v1.25.0/bin/linux/amd64/kube-proxy
kube_proxy_binary=$DTKBASE/binaries/kube-proxy-v1.25.0

# extras
metrics_server_download=https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.2/components.yaml
metrics_server_manifest=$DTKBASE/binaries/metrics-server-components.yaml
kubernetes_dashboard_download=https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
kubernetes_dashboard_manifest=$DTKBASE/binaries/kubernetes-dashboard-recommended-v2.7.0.yaml
# do cilium/hubble differently - see the README with the install script
#cilium_yaml_download=https://raw.githubusercontent.com/cilium/cilium/1.9.4/install/kubernetes/quick-install.yaml
#cilium_yaml=cilium-1.9.4-quick-install.yaml
#hubble_yaml_download=https://raw.githubusercontent.com/cilium/cilium/1.9.4/install/kubernetes/quick-hubble-install.yaml
#hubble_yaml=cilium-1.9.4-quick-hubble-install.yaml
kube_prometheus_download=https://github.com/prometheus-operator/kube-prometheus/archive/v0.10.0.tar.gz
kube_prometheus_binary=$DTKBASE/binaries/kube-prometheus-0.10.0.tar.gz
calico_tigera_download=https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/tigera-operator.yaml
calico_tigera_manifest=$DTKBASE/binaries/calico-v3.24.1-tigera-operator.yaml
calico_custom_resources_download=https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/custom-resources.yaml
calico_custom_resources_manifest=$DTKBASE/binaries/calico-v3.24.1-custom-resources.yaml
openebs_hostpath_operator_manifest_download=https://raw.githubusercontent.com/openebs/charts/gh-pages/hostpath-operator.yaml
openebs_hostpath_operator_manifest=$DTKBASE/binaries/openebs-hostpath-operator.yaml
openebs_sc_manifest_download=https://openebs.github.io/charts/openebs-lite-sc.yaml
openebs_sc_manifest=$DTKBASE/binaries/openebs-lite-sc.yaml

#
# Verifies the existence of all the filesystem objects required to provision a cluster. If arg1 is '1' then that means
# we are creating a template VM. In that case the OS and VBox Guest Additions ISOs are included in the check
#
function check_files() {
  local result=0
  local objects=($containerd_binary $cni_plugins_binary $runc_binary $crictl_binary\
    $etcd_gzip $kube_apiserver_binary $kube_controller_manager_binary $kube_scheduler_binary $kubelet_binary,$kubernetes_dashboard_manifest)
  if [[ "$1" == 1 ]]; then
    objects+=($guest_additions_path)
    if [[ $linux == "centos" ]]; then
      objects+=($centos_iso_path)
    else
      objects+=($rocky_iso_path)
    fi
  fi
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    objects+=($metrics_server_manifest)
  fi
  if [[ "$monitoring" == "kube-prometheus" ]]; then
    objects+=($kube_prometheus_binary)
  fi
  if [[ "$networking" == "kube-router" ]]; then
    objects+=($kube_router_yaml)
  fi
  if [[ "$networking" == "calico" ]]; then
    objects+=($kube_proxy_binary)
    objects+=($calico_tigera_manifest)
    objects+=($calico_custom_resources_manifest)
  fi
  if [[ "$storage" == "openebs" ]]; then
    objects+=($openebs_hostpath_operator_manifest)
    objects+=($openebs_sc_manifest)
  fi
#  if [[ "$networking" == "cilium" ]]; then
#    objects+=($cilium_yaml_download)
#    objects+=($hubble_yaml_download)
#  fi
  for f in "${objects[@]}"; do
    if ! [[ -f $f ]]; then
      echo "missing filesystem object: $f" >&2
      result=1
    else
      echo "OK: $f"
    fi
  done
  echo "$result"
}

#
# Verifies the existence of all the downloads required to provision a cluster. If arg1 is '1' then
# the OS and VBox Guest Additions ISOs are included in the check
#
function check_urls() {
  local result=0
  local objects=($etcd_download $kube_apiserver_download $kube_controller_manager_download $kube_scheduler_download\
    $crictl_download $runc_download $cni_plugins_download $containerd_download $kubelet_download, $kubernetes_dashboard_download)
  if [[ "$1" == 1 ]]; then
    objects+=($guest_additions_download)
    if [[ $linux == "centos" ]]; then
      objects+=($centos_iso_download)
    else
      objects+=($rocky_iso_download)
    fi
  fi
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    objects+=($metrics_server_download)
  fi
  if [[ "$monitoring" == "kube-prometheus" ]]; then
    objects+=($kube_prometheus_download)
  fi
  if [[ "$networking" == "kube-router" ]]; then
    objects+=($kube_router_yaml_download)
  fi
  if [[ "$networking" == "calico" ]]; then
    objects+=($kube_proxy_download)
    objects+=($calico_tigera_download)
    objects+=($calico_custom_resources_download)
  fi
  if [[ "$storage" == "openebs" ]]; then
    objects+=($openebs_hostpath_operator_manifest_download)
    objects+=($openebs_sc_manifest_download)
  fi
#  if [[ "$networking" == "cilium" ]]; then
#    objects+=($cilium_yaml)
#    objects+=($hubble_yaml)
#  fi
  for f in "${objects[@]}"; do
    if ! curl -sL $f -o /dev/null --head --fail; then
      echo "missing upstream resource: $f" >&2
      result=1
    else
      echo "OK: $f"
    fi
  done
  echo "$result"
}

function usage() {
  while IFS= read -r line; do
    echo "${line:4}"
  done <<< '
    dtk
    ---
    Creates a Kubernetes cluster on the desktop using VirtualBox and kickstart for hands-free CentOS
    or Rocky Linux installation. The cluster consists of three nodes: one controller and two workers.
    The nodes are configured with networking support for host-to-guest, guest-to-guest, and
    guest-to-internet.

    The script creates a "template" VM, and then clones that for each cluster node. The initial
    template VM creation runs Kickstart to install the OS, and also installs VirtualBox Guest Additions
    because that provides the ability to get the IP address from a running VM. Note - creating the template
    VM takes a fair bit of time, including the initial 60 seconds for the boot menu to time out and actually
    start the install.

    Usage: dtk [--host-network-interface=<interface>] [--host-only-network=<network>] [--vboxdir=</some/path>]
           [--networking=<type>] [--monitoring=<type>] [--create-template] [--single-node] [--template-name]
           [--linux=<centos|rocky>] [--storage=<type>] [--help] [--check-compatibility] [--verify=<files|upstreams>]
           [--up=<nodelist>] [--down=<nodelist>] [--delete=<nodelist>] [--sshto=<vmname>]

    Options:

      Cluster creation options:

      --vboxdir                  Required. The directory where you keep your VirtualBox VM files. The script uses
                                 the VBoxManage utility to create the VMs, which will in turn create a sub-directory
                                 under this directory for each VM. The directory must exist. The script will not
                                 create it.
      --host-network-interface   Specify this, or --host-only-network. If this, then the value is the name of the primary
                                 network interface on your machine. The scripts use this to configure the VirtualBox
                                 bridge network for each node VM. This option must be applied identically to the
                                 template VM and all clones.
      --host-only-network        Specify this, or --host-network-interface. If this, then the value is the left three
                                 octets of the host only network. This option configures NAT + host only networking
                                 mode. The scripts will create a new host only network and configure the cluster to
                                 use it for intra-cluster networking, and will configure NAT for the cluster to
                                 access the internet. This option must be applied identically to the template VM
                                 and all clones. (The parameter is only used when creating the template. The presence
                                 of the option is used when creating the clones.)
      --networking               Optional. Installs k8s networking. Current valid values are "calico", "kube-router",
                                 and "cilium". E.g.: --networking=calico. (Calico also installs kube-proxy. The
                                 other networking configurations are kube-proxyless.)
      --monitoring               Optional. Installs monitoring. Allowed values are "metrics.k8s.io", and
                                 "kube-prometheus".
      --create-template          Optional. First creates a template VM from which to clone all the cluster nodes.
                                 (This step by far takes the longest.) You have to do this at least once. Once the
                                 template is created, brings up the cluster by cloning the template for each node.
                                 If not specified, the script expects to find an existing VM to clone from.
      --template-name            Optional. Specifies the template name to create - or clone from. Default is
                                 "bingo" if not provided.
      --single-node              Optional. Creates a single node cluster. The default is to create one control
                                 plane node, and two workers. This option is useful to quickly test changes since
                                 it is faster to provision a single node.
      --linux                    Optional. Valid values are "centos" for CentOS Stream (the default) and "rocky"
                                 for Rocky Linux. Ignored unless --create-template is specified.
      --storage                  Optional. Installs a dynamic storage provisioner. This supports testing workloads
                                 that use PVs and PVCs. Presently, the only supported value is "openebs". E.g.:
                                 --storage=openebs

      Other optional options:

      --verify                   Looks for all the upstreams or filesystem objects used by the script. Valid options
                                 are "upstreams" and "files". If "upstreams", then the script does a curl HEAD request
                                 for each upstream (e.g. OS ISO, Kubernetes binaries, etc.). If "files", then
                                 the same check is performed for the downloaded filesystem objects. This is a useful
                                 option to see all the objects that are required to provision a cluster. The list is
                                 influenced by some other options. E.g. if you specify --monitoring=kube-prometheus,
                                 then that will add to the list of objects.
      --check-compatibility      Checks the installed versions of various utils used by the project (curl, kubectl,
                                 etc) against what the project has been tested on - and then exits, taking no
                                 further action. You should do this at least once.
      --up                       Takes a comma-separated list of VM names, and starts them all.
      --down                     Opposite of --up. Note - this is very low-tech at present: Order the args with the
                                 workers first and the controller last.
      --delete                   Tears down the cluster: Force stops all VMs and removes associated files from
                                 the directory specified in the --vboxdir option.
      --sshto                    SSHs to the named VM. E.g: "--sshto myvmname"
      --help                     Displays this help and exits.

    Examples:

      dtk --create-template --host-network-interface=enp0s31f6 --vboxdir=/sdb1/virtualbox --networking=calico

      Wires the VirtualBox bridge network to interface "enp0s31f6", uses the "/sdb1/virtualbox" directory
      for all VM files. Creates a template to clone the nodes from. Installs calico networking

      dtk --host-only-network=50.1.1 --vboxdir=/sdb1/virtualbox --single-node --networking=calico

      Uses the template VM created by a prior invocation. Configures host-only plus NAT networking. Creates a
      single-node cluster.

      dtk --up=vm1,vm2,vm3

      Starts the VMs for the default three-node cluster created by the script.
    '
}

#
# option parsing helper
#
function opt_val() {
  opt="$1"
  if [[ "$opt" == =* ]]; then
    echo "${opt:1}"
  else
    echo "$opt"
  fi
}

#
# parses command line parameters and sets script variables from them
#
function parse_args() {
  if [[ "$#" -eq 0 ]]; then
    usage
    exit 1
  fi
  local parsed
  local script_name=$(basename "$0")
  parsed=$(getopt --options "" --longoptions $long_opts -n $script_name -- "$@")
  eval set -- "$parsed"
  while true; do
    case "$1" in
      --help)
        usage
        exit 0
        ;;
      --host-network-interface)
        host_network_interface=$(opt_val "$2")
        shift 2
        ;;
      --host-only-network)
        host_only_network=$(opt_val "$2")
        shift 2
        ;;
      --vboxdir)
        vboxdir=$(opt_val "$2")
        shift 2
        ;;
      --networking)
        networking=$(opt_val "$2")
        if [[ "$networking" != "kube-router" ]] && [[ "$networking" != "cilium" ]] && [[ "$networking" != "calico" ]]; then
          echo "unsupported parameter value for --networking option: $networking"
          exit 1
        fi
        shift 2
        ;;
      --check-compatibility)
        check_compatibility=1
        shift
        ;;
      --create-template)
        create_template=1
        shift
        ;;
      --monitoring)
        monitoring=$(opt_val "$2")
        if [[ "$monitoring" != "metrics.k8s.io" ]] && [[ "$monitoring" != "kube-prometheus" ]]; then
          echo "unsupported parameter value for --monitoring option: $monitoring"
          exit 1
        fi
        shift 2
        ;;
      --storage)
        storage=$(opt_val "$2")
        if [[ "$storage" != "openebs" ]]; then
          echo "unsupported parameter value for --storage option: $storage"
          exit 1
        fi
        shift 2
        ;;
      --single-node)
        single_node=1
        shift
        ;;
      --up)
        up=$(opt_val "$2")
        shift 2
        ;;
      --down)
        down=$(opt_val "$2")
        shift 2
        ;;
      --delete)
        delete=$(opt_val "$2")
        shift 2
        ;;
      --verify)
        verify=$(opt_val "$2")
        if [[ "$verify" != "upstreams" ]] && [[ "$verify" != "files" ]]; then
          echo "unsupported parameter value for --verify option: $verify"
          exit 1
        fi
        shift 2
        ;;
      --sshto)
        sshto=$(opt_val "$2")
        shift 2
        ;;
      --template-name)
        template_name=$(opt_val "$2")
        shift 2
        ;;
      --linux)
        linux=$(opt_val "$2")
        if [[ "$linux" != "centos" ]] && [[ "$linux" != "rocky" ]]; then
          echo "unsupported parameter value for --linux option: $linux"
          exit 1
        fi
        shift 2
        ;;
      --)
        shift
        break
        ;;
    esac
  done

  #debug
  #echo "host_network_interface=$host_network_interface"
  #echo "host_only_network=$host_only_network"
  #echo "vboxdir=$vboxdir"
  #echo "check_compatibility=$check_compatibility"
  #echo "create_template=$create_template"
  #echo "networking=$networking"
  #echo "monitoring=$monitoring"
  #echo "storage=$storage"
  #echo "single_node=$single_node"
  #echo "verify=$verify"
  #echo "template_name=$template_name"
  #echo "linux=$linux"

  if [[ $# -ne 0 ]]; then
    echo "Unsupported command line option(s): $@"
    exit 1
  fi
}

# entry point

parse_args "$@"

if [[ $check_compatibility -eq 1 ]]; then
  echo "Checking version compatibility"
  xec verify-prereqs
  exit 0
elif [[ "$verify" == "upstreams" ]]; then
  check_urls $create_template
  exit 0
elif [[ "$verify" == "files" ]]; then
  check_files $create_template
  exit 0
elif [[ ! -z "$up" ]]; then
  IFS=',' read -ra vms <<< "$up"
  for vm in "${vms[@]}"; do
    VBoxManage startvm $vm
  done
  xec show-ssh $DTKBASE/generated/kickstart/id_ed25519 "${vms[@]}"
  exit 0
elif [[ ! -z "$down" ]]; then
  IFS=',' read -ra vms <<< "$down"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm"
    VBoxManage controlvm $vm acpipowerbutton
    xec wait-vm $vm --stopped
  done
  exit 0
elif [[ ! -z "$delete" ]]; then
  IFS=',' read -ra vms <<< "$delete"
  for vm in "${vms[@]}"; do
    echo "shutting down $vm if running"
    VBoxManage controlvm $vm poweroff &>/dev/null && xec wait-vm $vm --stopped || echo "(not running)"
    echo "removing $vm"
    sleep 2s
    VBoxManage unregistervm $vm --delete
  done
  exit 0
elif [[ ! -z "$sshto" ]]; then
  ip=$(xec get-vm-ip $sshto)
  ssh -i $DTKBASE/generated/kickstart/id_ed25519 root@$ip
fi

if [[ -z "$host_network_interface" ]] && [[ -z "$host_only_network" ]]; then
  echo "either --host-network-interface or --host-only-network is required"
  exit 1
elif [[ ! -z "$host_network_interface" ]] && [[ ! -z "$host_only_network" ]]; then
  echo "--host-network-interface and --host-only-network are exclusive of each other"
  exit 1
elif [[ -z "$vboxdir" ]]; then
  echo "--vboxdir is a required option"
  exit 1
elif [[ ! -d $vboxdir ]]; then
  echo "directory for virtualbox VMs does not exist: $vboxdir"
  exit 1
fi

if [[ ! -z "$host_only_network" ]]; then
  controller_octet=200
  worker1_octet=201
  worker2_octet=202
fi

# create directories to generate various files into
mkdir -p $DTKBASE/generated/kickstart $DTKBASE/generated/kubeconfig\
         $DTKBASE/generated/cert $DTKBASE/generated/hostonly-netcfg $DTKBASE/generated/iso

# Create a template VM to clone the nodes from. This step also generates the SSH keys into the 'generated/kickstart'
# directory that are used all over the place to interact with the node VMs via ssh/scp as they are being bootstrapped,
# and that you will use to SSH into the VMs after the cluster is provisioned. If using host only networking, the
# 'create-template-vm' script will also create a host only VirtualBox network which will be used to initialize the
# template. This host only network name will be copied into every VM cloned from the template. So every cloned VM
# will be on the same host only network.
#
# Script args are validated above so caller never specifies both bridge (by virtue of the --host-network-interface
# option) AND host only (by virtue of the --host-only-network option)

if [[ $create_template -eq 1 ]] ; then
  echo "creating a template VM"

  if [[ $linux == "centos" ]]; then
    linux_iso_download=$centos_iso_download
    linux_iso_path=$centos_iso_path
  else
    linux_iso_download=$rocky_iso_download
    linux_iso_path=$rocky_iso_path
  fi
  xec create-template-vm\
   --template-vmname=$template_name\
   --linux-iso-download=$linux_iso_download\
   --linux-iso-path=$linux_iso_path\
   --guest-additions-download=$guest_additions_download\
   --guest-additions-path=$guest_additions_path\
   --host-network-interface=$host_network_interface\
   --host-only-network=$host_only_network\
   --vboxdir=$vboxdir
fi

# gen-root-ca generates $DTKBASE/generated/cert/ca.pem and ca-key.pem used throughout to configure TLS

echo "generating root CA"
xec gen-root-ca

echo "provisioning a controller node"

xec provision-controller\
 --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
 --template-vmname=$template_name\
 --host-only-network=$host_only_network\
 --host-only-octet=$controller_octet\
 --vboxdir=$vboxdir\
 --controller-hostname=doc\
 --controller-ram=8192\
 --controller-cpu=4\
 --controller-disk=20000\
 --etcd-download=$etcd_download\
 --etcd-gzip=$etcd_gzip\
 --kube-apiserver-download=$kube_apiserver_download\
 --kube-apiserver-binary=$kube_apiserver_binary\
 --kube-controller-manager-download=$kube_controller_manager_download\
 --kube-controller-manager-binary=$kube_controller_manager_binary\
 --kube-scheduler-download=$kube_scheduler_download\
 --kube-scheduler-binary=$kube_scheduler_binary\
 --ca-cert=$DTKBASE/generated/cert/ca.pem\
 --ca-key=$DTKBASE/generated/cert/ca-key.pem

echo "provisioning worker nodes"

controller_ip=$(xec get-vm-ip doc)

# make the controller a worker as well (use admin.kubeconfig created by provision-controller
xec configure-worker\
 --controller-ip=$controller_ip\
 --controller-hostname=doc\
 --worker-hostname=doc\
 --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
 --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
 --crictl-download=$crictl_download\
 --crictl-binary=$crictl_binary\
 --runc-download=$runc_download\
 --runc-binary=$runc_binary\
 --cni-plugins-download=$cni_plugins_download\
 --cni-plugins-binary=$cni_plugins_binary\
 --containerd-download=$containerd_download\
 --containerd-binary=$containerd_binary\
 --kubelet-download=$kubelet_download\
 --kubelet-binary=$kubelet_binary\
 --pod-cidr=10.200.1.0/24\
 --node-labels=controller,worker\
 --ca-cert=$DTKBASE/generated/cert/ca.pem\
 --ca-key=$DTKBASE/generated/cert/ca-key.pem

if [[ $single_node -eq 0 ]]; then
  cluster_vms+=(ham)
  cluster_vms+=(monk)
  xec provision-workers\
   --worker=ham,8192,2,10.200.2.0/24,$worker1_octet\
   --worker=monk,8192,2,10.200.3.0/24,$worker2_octet\
   --host-only-network=$host_only_network\
   --controller-ip=$controller_ip\
   --controller-hostname=doc\
   --template-vmname=$template_name\
   --vboxdir=$vboxdir\
   --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
   --crictl-download=$crictl_download\
   --crictl-binary=$crictl_binary\
   --runc-download=$runc_download\
   --runc-binary=$runc_binary\
   --cni-plugins-download=$cni_plugins_download\
   --cni-plugins-binary=$cni_plugins_binary\
   --containerd-download=$containerd_download\
   --containerd-binary=$containerd_binary\
   --kubelet-download=$kubelet_download\
   --kubelet-binary=$kubelet_binary\
   --node-labels=worker\
   --ca-cert=$DTKBASE/generated/cert/ca.pem\
   --ca-key=$DTKBASE/generated/cert/ca-key.pem
fi

echo "configuring /etc/hosts in VMs"
xec configure-etc-hosts $DTKBASE/generated/kickstart/id_ed25519 "${cluster_vms[@]}"

echo "installing pod networking"

if [[ "$networking" == "kube-router" ]]; then
  xec install-kube-router\
   --controller-ip=$controller_ip\
   --kube-router-yaml-download=$kube_router_yaml_download\
   --kube-router-yaml=$kube_router_yaml\
   --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
elif [[ "$networking" == "cilium" ]]; then
  xec install-cilium-networking\
   --controller-ip=$controller_ip\
   --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
elif [[ "$networking" == "calico" ]]; then
  nodes="${cluster_vms[@]}"
  # convert array (x y z) to comma-delimited string "x,y,z":
  nodes=${nodes// /,}
  xec install-kube-proxy\
   --controller-ip=$controller_ip\
   --kube-proxy-download=$kube_proxy_download\
   --kube-proxy-binary=$kube_proxy_binary\
   --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
   --nodes=$nodes\
   --ca-cert=$DTKBASE/generated/cert/ca.pem\
   --ca-key=$DTKBASE/generated/cert/ca-key.pem
  xec install-calico-networking\
   --calico-tigera-download=$calico_tigera_download\
   --calico-tigera-binary=$calico_tigera_manifest\
   --calico-custom-resources-download=$calico_custom_resources_download\
   --calico-custom-resources-manifest=$calico_custom_resources_manifest\
   --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
   --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
   --cluster-cidr=10.200.0.0/16\
   --nodes=$nodes
fi

echo "installing cluster DNS"

xec install-coredns\
 --replicas=${#workers[@]}\
 --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig

if [[ -n "$monitoring" ]]; then
  echo "installing Monitoring"
  if [[ "$monitoring" == "metrics.k8s.io" ]]; then
    xec install-metrics-server\
     --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
     --metrics-server-download=$metrics_server_download\
     --metrics-server-manifest=$metrics_server_manifest
  elif [[ "$monitoring" == "kube-prometheus" ]]; then
    nodes="${cluster_vms[@]}"
    nodes=${nodes// /,}
    xec install-kube-prometheus\
     --nodes=$nodes\
     --kube-prometheus-download=$kube_prometheus_download\
     --kube-prometheus-binary=$kube_prometheus_binary\
     --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
  fi
fi

if [[ -n "$storage" ]]; then
  echo "installing a storage provisioner"
  if [[ "$storage" == "openebs" ]]; then
    xec install-openebs\
     --priv-key=$DTKBASE/generated/kickstart/id_ed25519\
     --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
     --openebs-hostpath-operator-manifest-download=$openebs_hostpath_operator_manifest_download\
     --openebs-hostpath-operator-manifest=$openebs_hostpath_operator_manifest\
     --openebs-sc-manifest-download=$openebs_sc_manifest_download\
     --openebs-sc-manifest=$openebs_sc_manifest
  fi
fi

echo "installing Kubernetes Dashboard"

xec install-kubernetes-dashboard\
 --controller-ip=$controller_ip\
 --admin-kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig\
 --kubernetes-dashboard-download=$kubernetes_dashboard_download\
 --kubernetes-dashboard-manifest=$kubernetes_dashboard_manifest

echo
echo "finished provisioning cluster. To interact with the cluster:"
echo "  export KUBECONFIG=$DTKBASE/generated/kubeconfig/admin.kubeconfig"
echo

xec show-ssh $DTKBASE/generated/kickstart/id_ed25519 "${cluster_vms[@]}"
