{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Desktop Kubernetes is a Linux Bash CLI that provisions a desktop Kubernetes cluster using KVM or VirtualBox - with each cluster node consisting of a guest VM on your desktop running Alma, CentOS, or Rocky Linux.</p> <p>There are many fine and innovative Kubernetes distros. This distro has just a few goals:</p> <ol> <li>Create a cluster consisting of VMs. Since many of us troubleshoot Kubernetes running in VMs, this seems natural.</li> <li>Create and run the cluster on the desktop workstation rather that requiring to buy additional hardware or rent virtualized infrastructure.</li> <li>Use pure upstream binaries without any additional packaging or alteration.</li> <li>Create a cluster provisioning process that is simple and repeatable.</li> </ol> <p>The goal of this distro is to create stable, reliable, production-grade VM-based development clusters to support at-home or localized desktop development of Kubernetes workloads, and experimentation with CNCF Kubernetes workloads.</p> <p></p> <p>Desktop Kubernetes is the 57 Chevy of Kubernetes distros: you can take it apart and put it back together with just a few Linux console tools: <code>bash</code>, <code>curl</code>, <code>ssh</code>, <code>scp</code>, <code>tar</code>, <code>openssl</code>, <code>helm</code>, <code>yq</code>, kvm tools (<code>virsh</code>, <code>virt-install</code>), and <code>kubectl</code>. That being said, v1.35.0 of this distribution is Kubernetes Certified. See: CNCF Landscape.</p> <p>The project consists of a number of bash scripts and supporting manifests / config files. The design is documented in the Design Section.</p> <p>This project started as a way to automate the steps in Kelsey Hightower's Kubernetes The Hard Way - just to see if I could. But over time it matured and, at this point I rely on it for all my local Kubernetes development. I use it on Ubuntu 24+ systems with 64 gigs of RAM and 6+ hyper-threaded processors. I can run a three-node cluster with each VM having 2-3 CPUs and 8+ gigs of RAM.</p> <p>Desktop Kubernetes can provision clusters with one, two, three, or more nodes. The only limits are the memory and CPU of your desktop computer.</p>"},{"location":"add-ons/","title":"Addons","text":"<p>The Add-ons are Helm workloads that are generally considered useful to a wide audience.</p> <p>Desktop Kubernetes supports Add-ons as follows:</p> <ol> <li>The <code>config.yaml</code> specifies an Add-On name.</li> <li>That Add-On name has to exactly match a subdirectory under the <code>scripts/addons</code> directory.</li> <li>The CLI loops through all enabled add-ons and simply runs a script named <code>install</code> in each such directory matching the name of the enabled Add-On.</li> </ol>"},{"location":"add-ons/#add-on-install-scripts","title":"Add-On install scripts","text":"<p>Each Add-On install script does the same things:</p> <ol> <li>Downloads the Add-On tarball using a hard-coded version in the Add-On <code>install</code> script.</li> <li>Helm-installs the Add-On.</li> </ol> <p>Some Add-Ons are more complex because because the Add-On might require:</p> <ol> <li>Templating Helm values</li> <li>Manipulating the guest VMs. (E.g.: Calico requires copying a config file to each host and restarting NetworkManager)</li> </ol> <p>For those you will see more logic in the Add-On <code>install</code> script. In general, the approach is for the main CLI <code>dtk</code> to know nothing about the Add-Ons except how to find the <code>install</code> script.</p> <p>See the Components and Versions Section for a list of the Add-Ons and versions.</p>"},{"location":"command-line/","title":"The Command Line","text":"<p>The <code>dtk</code> CLI takes one mandatory command, and then an optional sub-command or operand, and flags. As stated elsewhere in the documentation, most cluster configuration is read from a configuration yaml file. See the configuration section for more information on that.</p>"},{"location":"command-line/#usage","title":"Usage","text":"<pre><code>dtk command [sub-command] [flags]\n</code></pre>"},{"location":"command-line/#commands","title":"Commands","text":"Command Sub-cmd / Operand Description cluster create Creates a cluster per the configuration yaml file. up Starts all the VMs in configuration yaml. down Stops all the VMs in configuration yaml. delete Deletes a cluster by deleting all the VMs in configuration yaml. verify upstreams Does a curl HEAD request on all core k8s components, ISOs that will be downloaded to provision a cluster. files Reports on which core k8s components have already been downloaded to the file system. install-addon The addon name Install an add-on from the scripts/addons directory. See the Add-Ons section for more detail. check-tools n/a Reports on the presence or absence, and version, of the tools used by the CLI to provision a cluster. version n/a Displays this CLI version. help n/a Displays this help."},{"location":"command-line/#options","title":"Options","text":"Option Description <code>--config</code> The path to a configuration yaml file that specifies the cluster options. If not provided, uses the configuration yaml file in the same directory as the CLI. <code>--create-template</code> Overrides the setting specified in the configuration yaml. Allowed values are <code>true</code> and <code>false</code>. <code>--create-vms</code> Create VMs. Allowed values are <code>true</code> and <code>false</code>. Default is true. If false, then the VMs in the configuration yaml file must be up and running, and the installer will simply install Kubernetes on them. <code>--help</code> Same as the help command."},{"location":"command-line/#examples","title":"Examples","text":"<pre><code>./dtk cluster create --config mycluster.yaml\n</code></pre> <p>Creates a cluster as defined in the <code>mycluster.yaml</code> file.</p> <pre><code>./dtk cluster create --create-template=false\n</code></pre> <p>Like above except re-uses the existing template from a prior cluster creation, and uses the configuration yaml in the CLI dir. (The configuration yaml specifies the name of the template VM.)</p> <pre><code>./dtk cluster up\n</code></pre> <p>Starts the VMs listed in the configuration yaml in the CLI dir.</p> <pre><code>./dtk install-addon external-dns\n</code></pre> <p>Installs the external-dns add-on from scripts/addons.</p>"},{"location":"components-and-versions/","title":"Components and Versions","text":"<p>This project has been tested with the tools, components and versions shown in the tables below. Where the Category column says host - you are responsible to install those. This project tries not to alter your environment and so this project will use whatever is present and will not mutate your system. Everything else in the table is downloaded by the project but is only installed into the VMs created by the project.</p> <p>To install with different Kubernetes component versions, change the corresponding version variable (e.g. <code>K8S_VER</code> or <code>GUEST_ADDITIONS_VER</code>) in the <code>artifacts</code> file. The install different Add-On versions, modify the version constant in the relevant Add-On <code>install</code> script. (More on that below.)</p> <p>This shows what I've tested with. Most likely, minor version differences will still work with Desktop Kubernetes.</p>"},{"location":"components-and-versions/#components","title":"Components","text":"<p>The categories are:</p> <ol> <li>Host: These tools must be installed on your desktop (by you.)</li> <li>Guest VM:  These are installed by Desktop Kubernetes into the Guest VMs that make up the cluster.</li> <li>Kubernetes: These run Kubernetes in the Guest VMs.</li> </ol> Category Component Version VirtualBox Only KVM Only Host Linux desktop Ubuntu 22.04.5 LTS - - Host openssl 3.0.2 - - Host openssh OpenSSH_8.9p1 - - Host genisoimage (used to create a VirtualBox kickstart ISO) 1.1.11 Yes - Host Virtual Box / VBoxManage 7.0.10 Yes - Host helm v4.1.1 - - Host kubectl (client only) v1.35.0 - - Host curl 7.81.0 - - Host yq 4.40.5 - - Host virt-install / virt-clone 4.0.0 - Yes Host virsh 8.0.0 - Yes Host Hydrophone v0.7.0 - - Guest VM Centos ISO Stream-9-latest-x86_64 - - Guest VM Rocky Linux ISO 8.10 - - Guest VM Alma Linux ISO 8.10 and 9.7 (9.7 is the default) - - Guest VM Virtual Box Guest Additions ISO 7.0.18 Yes - Kubernetes kube-apiserver 1.35.0 - - Kubernetes kube-controller-manager 1.35.0 - - Kubernetes kube-scheduler 1.35.0 - - Kubernetes kubelet 1.35.0 - - Kubernetes kube-proxy (if installed) 1.35.0 - - Kubernetes etcd v3.6.7 - - Kubernetes crictl v1.35.0 - - Kubernetes runc v1.4.0 - - Kubernetes cni plugins v1.9.0 - - Kubernetes containerd 2.1.4 - - <p>The Virtual Box Guest Additions ISO enables getting the IP address of a VM.</p>"},{"location":"components-and-versions/#add-ons","title":"Add-ons","text":"<p>To install different add-on versions change the version in the <code>scripts/addons</code> directory. (Version updates sometimes require <code>values.yaml</code> changes and other tweaks to the install logic.)</p> Add-on Chart Version Note Calico networking (Tigera Operator) v3.31.3 Cert Manager v1.19.2 Cilium networking 1.18.5 CoreDNS 1.45.0 External DNS 1.19.0 Headlamp 0.40.0 Ingress NGINX Controller 4.14.1 (deprecated) Kube Prometheus Stack 80.6.0 Kubernetes Dashboard 7.14.0 (deprecated) Metrics Server 3.13.0 NFS Subdir External Provisioner 4.0.18 Nginx Gateway Fabric 2.4.1 OpenEBS Local PV Provisioner 4.4.0 Vcluster 0.30.4"},{"location":"configuration/","title":"Configuration","text":"<p>The project includes a <code>config.yaml</code> file in the repo root that specifies the cluster configuration. The file is structured into the following sections:</p> Section Description <code>addons</code> A list of cluster add-ons to install, e.g.: CNI, CoreDNS, etc. <code>k8s</code> Kubernetes cluster configuration. <code>dns</code> Configuration for the External DNS Add-On. <code>kvm</code> KVM configuration. <code>vbox</code> VirtualBox configuration. <code>vm</code> VM configuration (common to KVM &amp; VirtualBox.) <code>vms</code> A list of VMs to create for the cluster, and their characteristics. <code>cluster</code> Populated by Desktop Kubernetes when a new cluster is provisioned. <p>There is one top-level key with no subkeys: <code>virt</code>. This key determines whether KVM or VirtualBox will be used to provision and manage the VMs. The options are <code>virtualbox</code> and <code>kvm</code>. The default value is <code>kvm</code>. The reason is that KVM is significantly faster (and simpler) to provision VMs than VirtualBox.</p>"},{"location":"configuration/#the-addons-section","title":"The <code>addons</code> Section","text":"<p>This section enables or disables the Add-Ons included with the CLI. See the Add-Ons section for more details. These are the default values:</p> Key Enabled by default? <code>calico</code> \u2718 <code>cilium</code> \u2714 <code>coredns</code> \u2714 <code>cert-manager</code> \u2714 <code>external-dns</code> \u2718 <code>headlamp</code> \u2714 <code>ingress-nginx</code> \u2718 <code>kube-prometheus-stack</code> \u2714 <code>kubernetes-dashboard</code> \u2718 <code>metrics-server</code> \u2714 <code>nfs-provisioner</code> \u2718 <code>nginx-gateway-fabric</code> \u2714 <code>openebs</code> \u2714 <code>vcluster</code> \u2718 <p>Note</p> <p>The Add-Ons are listed in the order in which they are installed. This is important. Notice that the CNI is first, followed by Core DNS for service IP address resolution. The order after that is unimportant.</p>"},{"location":"configuration/#the-k8s-section","title":"The <code>k8s</code> Section","text":"<p>The <code>k8s</code> section has configuration settings that configure Kubernetes, independent of the virtualization that you choose.</p> Key Description <code>k8s.containerized-cplane</code> If specified, creates the control plane components as static pods on the controller VM like kubeadm, RKE2, et. al. (By default, Desktop Kubermetes creates the control plane components as as systemd units.) Allowed values: <code>all</code>, or any of: <code>etcd</code>, <code>kube-apiserver</code>, <code>kube-proxy</code>, <code>kube-scheduler</code>, <code>kube-controller-manager</code> (comma-separated.) E.g.: <code>etcd,kube-apiserver</code> <code>k8s.cluster-cidr</code> Configures CIDR range for Pods. This is applied to the <code>kube-controller-manager</code>. (Be aware of <code>--node-cidr-mask-size...</code> args which you can't override at this time.) <code>k8s.cluster-dns</code> Ignored - not yet implemented. <code>k8s.kube-proxy</code> If true, you can run the cluster without Calico or Cilium (or other CNI) using the default CNI configuration that is established by <code>scripts/worker/containerd/install-containerd</code>. <code>k8s.containerd-mirror</code> Supports configuring <code>containerd</code> to mirror to a different registry. The example in the yaml has all images mirrored to a distribution server on 192.168.0.49:8080. Background: I use my own caching pull-only, pull-through OCI distribution server running as a systemd service on my desktop to mitigate DockerHub rate limiting. See the example immediately below. <p>Static pod container images for the containerized control plane per: https://kubernetes.io/releases/download/</p>"},{"location":"configuration/#configuring-containerd","title":"Configuring <code>containerd</code>","text":"<p>You can configure <code>containerd</code> to mirror with the following in the <code>config.yaml</code> file. Mirroring is disabled by default. You enable it by setting <code>enabled: true</code> and specifying the name and configuration. In the example, all images are mirrored (<code>name: _default</code> which is a special value recognized by <code>containerd</code>) and the mirror is http://192.168.0.49:8080. <pre><code>k8s:\n  containerd-mirror:\n    enabled: true\n    name: _default\n    config: |\n      [host.\"http://192.168.0.49:8080\"]\n        capabilities = [\"pull\", \"resolve\"]\n        skip_verify = true\n</code></pre></p> <p>With the configuration above, as Desktop Kubernetes is installing <code>containerd</code> into each guest, it also configures mirroring at the same time.</p>"},{"location":"configuration/#the-dns-section","title":"The <code>dns</code> Section","text":"<p>The <code>dns</code> section configures the External-DNS Add-On. Desktop Kubernetes uses the External-DNS Add-On to watch <code>Ingress</code> resources and configure <code>/etc/hosts</code> so that when you create an ingress with a host name, the host name is immediately DNS-resolveable. This means that after creating an ingress, you can use the address in your browser.</p> <p>This requires you to run the included External DNS webhook server. See the External DNS section for details.</p> Key Description <code>domain</code> This is a comma-separated list of domain names that the webhook server will manage. E.g.: <code>dtk.io,mydomain.com</code>. <code>host-ip</code> The IP address of your desktop. This will be configured into the <code>external-dns</code> Helm chart as the URL of the webhook. <code>webhook-port</code> The port address of the webhook server running on your desktop. This is also configured into the <code>external-dns</code> Helm chart."},{"location":"configuration/#the-vbox-section","title":"The <code>vbox</code> Section","text":"<p>This section is only used if <code>virt = virtualbox</code></p> Key Description <code>vbox.host-network-interface</code> The name of the primary network interface on your machine. The scripts use this to configure the VirtualBox bridge network for each guest VM. Important: you must specify this consistently when creating the template, and when creating a cluster from the template. The reason is that the option configures settings at the VM level that then propagate into the guest OS. Since guests are cloned from the template, the guest networking has to be defined consistently with the template. This config is mutually exclusive with <code>vbox.host-only-network</code>. <code>vbox.host-only-network</code> The left three octets of the network. E.g. <code>192.168.56</code>. (For some additional information on this address, see: the VirtualBox docs section on \"Host-Only Networking\".) This option configures NAT + host only networking mode. The scripts will create a new host only network and configure the cluster to use it for intra-cluster networking, and will configure NAT for the cluster to access the internet. See important note in the table entry immediately above regarding VBox networking type. This config is mutually exclusive with <code>vbox.host-network-interface</code>. <code>vbox.vboxdir</code> The directory where you keep your VirtualBox VM files. The script uses the <code>VBoxManage</code> utility to create the VMs, which will in turn create a sub-directory under this directory for each VM. If empty, the script will get the value from VirtualBox. The directory must exist. The script will not create it. <code>vbox.kickstart</code> Specifies the name of the kickstart file to configure the OS. The file has to be in the <code>kickstarts</code> directory. The default is <code>vbox.text.ks.cfg</code> which is a non-graphical install."},{"location":"configuration/#the-kvm-section","title":"The <code>kvm</code> Section","text":"<p>This section is only used if <code>virt = kvm</code></p> Key Description <code>kvm.network</code> This is set to <code>nat</code> in the configuration file. This setting is actually ignored because NAT is the only KVM networking option currently implemented but stating that in the configuration makes it more self-documenting. <code>kvm.kickstart</code> The kickstart file used when creating a template VM. Kickstart files are in the <code>kickstarts</code> directory of the project. The default is <code>kvm.text.ks.cfg</code>. <code>kvm.os-variant</code> Has to align with OS ISO. (Values from <code>virt-install --os-variant list</code>.) Default is <code>almalinux9</code>."},{"location":"configuration/#the-vm-section","title":"The <code>vm</code> Section","text":"Key Description <code>vm.linux</code> Determines the Linux variant.  Valid values are <code>alma9</code> for Alma 9.7 (the default), <code>alma8</code> for Alma 8.10, <code>centos9</code> for CentOS 9 Stream, and <code>rocky</code> for Rocky Linux. Ignored unless <code>vm.create-template</code> is specified. CentOS and Rocky are un-tested. (It's on the to-do list.) <code>vm.create-template</code> Values are <code>true</code> (the  default) or <code>false</code>. Causes the script to create a template VM before bringing up the cluster. (This step by far takes the longest.) The template is then used to clone all the cluster nodes. If <code>false</code>, the script expects to find an existing VM to clone from per the <code>vm.template-vmname</code> setting. You can override this with command line arg <code>--create-template=true\\|false</code>. This setting must be <code>true</code> for the very first cluster you create - meaning - you have to create a template at least once. <code>vm.template-vmname</code> Specifies the template VM name to create - or clone from."},{"location":"configuration/#the-vms-section","title":"The <code>vms</code> Section","text":"<p>This section has a list of dictionaries. Each element in the list is a VM to create and to install the Kubernetes components in.</p> Key Description <code>vms</code> This is a list of VMs to create. Each VM in the list specifies the following keys: <code>vms[n].name</code> The VM Name. <code>vms[n].cpu</code> Number of CPUs. <code>vms[n].mem</code> Memory in MB. E.g.: <code>8192</code> = 8 gig. <code>vms[n].ip</code> The rightmost octet of the IP address for the host. Ignored unless <code>virt=virtualbox</code> and <code>vbox.host-only-network</code> is configured. So, for example, if <code>vbox.host-only-network</code> is <code>192.168.56</code> and this <code>ip</code> value is <code>200</code>. then the IP address assigned to the host-only interface in the VM is <code>192.168.56.200</code>. <code>vms[n].disk</code> Only supported if <code>virt=kvm</code> at this time. Resizes the disk to the specified number which is interpreted as Gi. <code>vms[n].pod-cidr</code> Used to configure CNI for containerd. As soon as Cilium or Calico are installed then this configuration is rendered inert."},{"location":"configuration/#the-cluster-section","title":"The <code>cluster</code> Section","text":"Key Description <code>cluster</code> Contains cluster information for the add-ons. Populated (overwritten) by <code>scripts/addons/install-addons</code>. Avoid editing this as it will be overwritten whenever a cluster is provisioned."},{"location":"conformance/","title":"Conformance","text":"<p>This section documents the process of running Desktop Kubernetes through the Kubernetes conformance tests and getting it certified. This guide assumes you have git cloned the repo and the repo root is the current working directory. The first part of this guide assumes you've git cloned this repo and your current working directory is the root of the cloned repo.</p> <p>This is presented purely as background information since the certification request can only come from the project fork. But you may wish to run the conformance tests on your own cluster provisioned by Desktop Kubernetes.</p>"},{"location":"conformance/#get-hydrophone","title":"Get Hydrophone","text":"<p>The project uses Hydrophone to run the conformance tests.</p> <pre><code>go install sigs.k8s.io/hydrophone@latest\n</code></pre>"},{"location":"conformance/#run-conformance-tests","title":"Run conformance tests","text":"<pre><code>hydrophone --conformance --parallel 5\n</code></pre> <p>The test results are generated into the current working directory.</p>"},{"location":"conformance/#move-test-results-to-submission-folder","title":"Move test results to submission folder","text":"<pre><code>rm conformance/conformance-submission/{e2e.log,junit_01.xml}\nmv e2e.log junit_01.xml conformance/conformance-submission\n</code></pre>"},{"location":"conformance/#clean-up-the-cluster","title":"Clean up the cluster","text":"<pre><code>hydrophone --cleanup\n</code></pre>"},{"location":"conformance/#edit-documents","title":"Edit documents","text":"<p>Update any new version numbers, etc.:</p> <ol> <li>The MKDocs documentation.</li> <li>The Project README.</li> <li>The <code>PRODUCT.yaml</code> and <code>README.md</code> files in <code>conformance/conformance-submission</code>.</li> <li>Git commit and push.</li> <li>Tag <code>desktop-kubernetes</code> with a tag matching the Kubernetes version. E.g.:    <code>git tag -a v1.35.0 -m \"Kubernetes 1.35.0 passes conformance using Hydrophone v0.7.0\"</code>.</li> <li>Git push the tag: <code>git push origin v1.35.0</code>.</li> </ol>"},{"location":"conformance/#conformance-fork","title":"Conformance fork","text":"<p>This section assumes you've forked the conformance repo per their guidance. E.g., my fork is https://github.com/aceeric/k8s-conformance.git. Set the working directory to where the repo is cloned on the file system.</p> <ol> <li>Sync the fork with the upstream (in the GitHub UI) so you're starting from the latest.</li> <li>Do a <code>git pull</code>.</li> <li>Create branch, e.g.: <code>git checkout -b v1.35-desktop-kubernetes</code>.</li> <li>Create directory, e.g.: <code>mkdir ./v1.35/desktop-kubernetes</code>.</li> <li>Populate the directory: <code>cp ~/projects/desktop-kubernetes/conformance/conformance-submission/* ./v1.35/desktop-kubernetes</code>.</li> <li>Verify:    <pre><code>$ ls -l ./v1.35/desktop-kubernetes\ntotal 2312\n-rw-rw-r-- 1 eace eace    8618 Dec 24 11:31 e2e.log\n-rw-rw-r-- 1 eace eace 2567261 Dec 24 11:31 junit_01.xml\n-rw-rw-r-- 1 eace eace     549 Dec 24 11:31 PRODUCT.yaml\n-rw-rw-r-- 1 eace eace    3441 Dec 24 11:31 README.md\n</code></pre></li> <li>Git add and commit to the branch with message AND signoff:    <pre><code>git commit -m 'Conformance results for v1.35/desktop-kubernetes\nSigned-off-by: Eric Ace &lt;24485843+aceeric@users.noreply.github.com&gt;'\n</code></pre></li> <li>Push to GitHub.</li> <li>Create a Pull Request to https://github.com/cncf/k8s-conformance from the branch in the fork. For detailed instructions, see creating-a-pull-request-from-a-fork on the GitHub Collaborating with pull requests documentation page.</li> </ol>"},{"location":"design/","title":"Design","text":"<p>This project might seem like a lot of bash code but it's actually pretty simple. There are about 30-odd primary shell scripts, and a number of helper scripts. This README focuses on the primary scripts.</p> <p>Desktop Kubernetes is 99.9% bash shell with a tiny smidgen of Python.</p>"},{"location":"design/#directory-structure","title":"Directory structure","text":"<p>The script directory structure is organized around related areas of functionality. The scripts generate numerous files as part of provisioning a cluster. These generated files are all placed into the <code>generated</code> directory in the project root. Most of these are purely transitory, except:</p> File Purpose <code>generated/kickstart/id_ed25519</code> This is the private key corresponding to the public key that Desktop Kubernetes adds to the template VM <code>authorized_keys</code> file. As long as the template VM is used to provision new Desktop Kubernetes clusters, this private key must be retained for ssh'ing into the cluster VMs. Desktop Kubernetes only generates the SSH keys when a template is created - and only if the keypair does not already exist. <code>generated/kubeconfig/admin.kubeconfig</code> This is the cluster admin kubeconfig. It is regenerated for each new cluster you provision. You need this kubeconfig to run <code>kubectl</code> commands against the cluster."},{"location":"design/#call-structure","title":"Call structure","text":"<p>All of the scripts in the call tree except for <code>dtk</code> and <code>artifacts</code> are in the <code>scripts</code> directory. All of the scripts are invoked by <code>dtk</code>. The structure below shows the call tree as the scripts are invoked to create a template VM, and then provision a cluster using the template. See the Narrative section that follows for a description of each numeric annotation:</p> <pre><code>dtk\n\u251c\u2500 source artifacts (1)\n\u2514\u2500 create-cluster\n   \u251c\u2500 scripts/helpers/download-objects (2)\n   \u2502  \u2514\u2500 scripts/helpers/download-obj\n   \u2502\n   \u251c\u2500 scripts/kvm/provision-vms (3) (if kvm)\n   \u2502  \u251c\u2500 scripts/kvm/create-template-vm\n   \u2502  \u2502  \u2514\u2500 scripts/vm/gen-ssh-keyfiles\n   \u2502  \u251c\u2500 scripts/kvm/clone-vm\n   \u2502  \u2514\u2500 scripts/kvm/configure-etc-hosts\n   \u2502\n   \u251c\u2500 scripts/virtualbox/provision-vms (3) (if vbox)\n   \u2502  \u251c\u2500 scripts/virtualbox/create-template-vm\n   \u2502  \u2502  \u251c\u2500 scripts/vm/gen-ssh-keyfiles\n   \u2502  \u2502  \u251c\u2500 scripts/os/gen-kickstart-iso\n   \u2502  \u2502  \u251c\u2500 scripts/virtualbox/create-vm\n   \u2502  \u2502  \u2514\u2500 scripts/virtualbox/install-guest-additions\n   \u2502  \u251c\u2500 scripts/virtualbox/clone-vm\n   \u2502  \u2502  \u2514\u2500 scripts/virtualbox/configure-hostonly-networking\n   \u2502  \u2502     \u2514\u2500 scripts/virtualbox/gen-hostonly-ifcfg-iso\n   \u2502  \u2514\u2500 scripts/virtualbox/configure-etc-hosts\n   \u2502\n   \u251c\u2500 scripts/cluster/gen-root-ca (4)\n   \u2502\n   \u251c\u2500 scripts/cluster/gen-core-k8s (5)\n   \u2502  \u251c\u2500 scripts/worker/configure-worker (5a)\n   \u2502  \u2502  \u251c\u2500 scripts/os/configure-firewall\n   \u2502  \u2502  \u251c\u2500 scripts/worker/misc/install-misc-bins\n   \u2502  \u2502  \u251c\u2500 scripts/worker/containerd/install-containerd\n   \u2502  \u2502  \u251c\u2500 scripts/worker/kubelet/install-kubelet\n   \u2502  \u2502  \u2514\u2500 scripts/worker/kube-proxy/install-kube-proxy\n   \u2502  \u2514\u2500 scripts/control-plane/configure-controller (5b)\n   \u2502     \u251c\u2500 scripts/os/configure-firewall\n   \u2502     \u251c\u2500 scripts/control-plane/etcd/install-etcd\n   \u2502     \u251c\u2500 scripts/control-plane/kube-apiserver/install-kube-apiserver\n   \u2502     \u251c\u2500 scripts/control-plane/kube-controller-manager/install-kube-controller-manager\n   \u2502     \u2514\u2500 scripts/control-plane/kube-scheduler/install-kube-scheduler\n   \u2502\n   \u2514\u2500 scripts/addons/install-addons (6)\n</code></pre>"},{"location":"design/#narrative","title":"Narrative","text":"<p>The <code>dtk</code> script is the top-level script. To provision a new cluster, the work is all done in the <code>create-cluster</code> script:</p> <ol> <li>The <code>artifacts</code> file is sourced, which defines all the upstream URLs and local filesystem locations for the core objects needed to provision the cluster. Then the the <code>create-cluster</code> script is called to provision the cluster.</li> <li>All the binaries, ISOs, manifests, and tarballs needed to provision the core cluster are downloaded into the <code>binaries</code> directory based on configuration options. E.g. if config specifies <code>linux: rocky</code> then <code>Rocky-X.X-x86_64-dvd.iso</code> (X.X based on whatever is hard-coded in the <code>artifacts</code> file.)</li> <li>All the VMs are created:<ul> <li>If config specifies <code>create-template: true</code> then ssh keys are generated, and a template VM is created using Kickstart and a CentOS / Alma / Rocky ISO depending on the <code>linux</code> selection. The ssh public key is copied into the VM in the <code>authorized-keys</code> file.</li> <li>The template VM (the one created in the prior step, or one that was already there identified by the <code>template-vmname</code> config) is cloned to create the VM(s) that comprise the Kubernetes cluster, so each VM has an identical configuration.</li> </ul> </li> <li>A root CA is generated for the cluster if one does not already exist. This CA is used to sign cluster certs throughout the remainder of the cluster provisioning process.</li> <li>The core Kubernetes cluster is created by installing the canonical Kubernetes components on each VM:<ul> <li>5a: Each worker gets a unique TLS cert/key for its <code>kubelet</code>, a few binaries: <code>crictl</code>, <code>runc</code>, and <code>cni plugins</code>, and of course the <code>kubelet</code> and <code>containerd</code>.</li> <li>5b: The controller is provisioned with cluster TLS, <code>etcd</code>, the <code>api server</code>, <code>controller manager</code>, and <code>scheduler</code>. This project runs with a single controller to minimize the desktop footprint.</li> </ul> </li> <li>The <code>install-addons</code> script is called. It walks its own directory and for each subdirectory that matches an entry in the <code>addons</code> section of the <code>config.yaml</code>, it looks for and invokes an <code>install</code> script in that directory to install the add-on.</li> </ol> <p>On completion, you have a functional Kubernetes cluster consisting of one or one or more guest VMs, the first of which is always a controller, and the remainder of which are workers.</p>"},{"location":"external-dns/","title":"External DNS Integration","text":"<p>In a typical corporate environment, when a Kubernetes cluster is provisioned using IaC for example, DNS is usually configured as part of cluster provisioning.</p> <p>On the desktop we can't modify DNS to route <code>abc.com</code> to the Kubernetes cluster VMs running on your local host. But a simple way to accomplish this is to update <code>/etc/hosts</code> with such an entry. But it's tedious to do that by hand whenever a new Ingress resources is deployed. So DTK has the ability to automate updating <code>/etc/hosts</code> to add Ingress host names when a new Ingress resource is created in the cluster. Since everything is running right on your desktop, this is a completely reasonable approach, despite being low-tech.</p> <p>DTK implements this using two components:</p> <ol> <li>The External DNS add-on.</li> <li>A Python HTTP server included in the project repo that implements the External DNS webhook provider interface (and a couple additional methods.) The External DNS Add-On installer will configure the External DNS workload in the cluster to call out to this Python server.</li> </ol> <p>For details on External DNS's webhook design, see their documentation.</p> <p>The steps described in this guide assume you are running the Python HTTP server found in the <code>hostsfile-server</code> directory of DTK - either as a <code>systemd</code> service (which is how I run it) or just from the command line while you're provisioning and running your cluster. Details on this are provided further down in this document.</p> <p>Important</p> <p>Since the External DNS workload running inside the Kubernetes cluster within a VM on your desktop will be making HTTP calls to the Python HTTP server running outside the VM on your desktop you might need to configure firewall rules on your desktop to allow that traffic in to your host from the guest(s).</p> <p>On my Ubuntu desktop, for example, I use Gufw and I have a global rule to allow traffic into my desktop from the same network. When KVM creates the VMs, it uses DHCP on my home network to assign an IP address in same CIDR block as my desktop. Your environment might differ.</p>"},{"location":"external-dns/#how-it-works","title":"How it works","text":"<p>The sequence diagram illustrates the design:</p> <pre><code>sequenceDiagram\n    developer-&gt;&gt;desktop-kubernetes: 1: Create cluster\n    desktop-kubernetes-&gt;&gt;hostsfile-server.py: 2: Manage domain 'dtk.io'\n    desktop-kubernetes-&gt;&gt;kubernetes: 3: Install external-dns with webhook provider\n    kubernetes-&gt;&gt;external-dns: 4: Create workload\n    external-dns-&gt;&gt;hostsfile-server.py: 5: Get domains\n    hostsfile-server.py-&gt;&gt;external-dns: 6: 'dtk.io'\n    developer-&gt;&gt;kubernetes: 7: Create Ingress with host 'foo.dtk.io'\n    external-dns-&gt;&gt;kubernetes: 8: Watch Ingress\n    external-dns-&gt;&gt;external-dns: 9: Read Ingress domain\n    external-dns-&gt;&gt;hostsfile-server.py: 10: 'foo.dtk.io' was added\n    hostsfile-server.py-&gt;&gt;/etc/hosts: 11: Update\n    developer-&gt;&gt;kubernetes: 12: curl https://foo.dtk.io</code></pre> <p>Narrative:</p> <ol> <li>When you create a new Kubernetes cluster you enable the <code>external-dns</code> Add-On, and you configure External DNS integration using the <code>dns</code> section of the <code>config.yaml</code>. Example:    <pre><code>dns:\n  domain: dtk.io\n  host-ip: 192.168.0.12\n  webhook-port: 5000\n</code></pre>    The yaml entries are:<ol> <li><code>domain: dtk.io</code>: Defines the list of managed domains. Only those domains will be managed in <code>/etc/hosts</code> by the Python HTTP server. All other domains will be ignored.</li> <li><code>host-ip: 192.168.0.12</code>: Your desktop IP address.</li> <li><code>webhook-port: 5000</code>: The port you're running the Python HTTP server on. The desktop IP address and this port are templated into the External DNS Helm values when it is installed by DTK. That's how External DNS inside the cluster can call the Python HTTP server running on your desktop.</li> </ol> </li> <li>DTK sends the specified domain (<code>dtk.io</code> in this case) and the IP address of the Kubernetes controller VM to the Python HTTP server. The server stores these.</li> <li>DTK installs <code>external-dns</code>, either because you uncommented it in the <code>addons</code> list in <code>config.yaml</code>, or you explicitly installed it with <code>./dtk install-addon external-dns</code>. As part of this step, your host IP and webhook port are templated into the <code>external-dns</code> Helm values.</li> <li>Kubernetes creates the <code>external-dns</code> workload.</li> <li>When <code>external-dns</code> starts it sees that it is configured to integrate with the <code>webhook</code> provider so it calls the webhook (the Python HTTP server) to get a list of managed domains.</li> <li>The Python HTTP server replies with <code>dtk.io</code>.</li> <li>You (the developer) create an Ingress with one of the rules specifying host <code>foo.dtk.io</code> and with an annotation that External DNS understands. (There is an example ingress below showing the annotation.)</li> <li>External DNS watches Ingresses.</li> <li>External DNS reads the host name from the Ingress annotation.</li> <li>External DNS sends the host name to the webhook (the Python HTTP server.)</li> <li>The Python HTTP server updates <code>/etc/hosts</code>.</li> <li>You can now curl https://foo.dtk.io (or <code>http</code> depending on how you configured the Ingress.)</li> </ol>"},{"location":"external-dns/#example-ingress","title":"Example Ingress","text":"<p>Observe that the annotation below has the same host name as the <code>host</code> in the <code>rules</code> list:  <code>foo.dtk.io</code>:</p> <pre><code>cat &lt;&lt;EOF | kubectl -n default apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/target: foo.dtk.io\n  name: test\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: foo.dtk.io\n    http:\n      paths:\n      - backend:\n          service:\n            name: test\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\nEOF\n</code></pre>"},{"location":"external-dns/#running-the-webook-server-by-hand","title":"Running the Webook Server by hand","text":"<p>If you don't want to install the webook as a <code>systemd</code> service you can just run it while you're running the Kubernetes cluster that is running the External DNS Add-On. The <code>external-dns</code> install script initializes the webhook server from the <code>config.yaml</code> when it installs the Add-On. So you can run the server manually before installing the Add-On. Run as <code>sudo</code> because the process updates <code>/etc/hosts</code>:</p> <pre><code>sudo hostsfile-server/hostsfile-server.py\n</code></pre>"},{"location":"external-dns/#restarting-the-webook-server","title":"Restarting the Webook Server","text":"<p>If you stop the server, it loses all its state. You can restart it with the configuration that it would normally receive from the <code>external-dns</code> Add-On install script. E.g.:</p> <pre><code>sudo hostsfile-server/hostsfile-server.py\\\n  --cluster-ip=192.168.122.167\\\n  --port=5000\\\n  --domains=dtk.io,mydomain.io\n</code></pre> <p>In the example above, the <code>192.168.122.167</code> ip address is the address of the controller node VM. This is the IP address that gets interpolated into <code>/etc/hosts</code> whenever a new host name is received by the webhook from External DNS.</p>"},{"location":"external-dns/#install-script","title":"Install script","text":"<p>When you install the <code>external-dns</code> Add-On either by enabling it in the <code>config.yaml</code> or manually via <code>--install-addon external-dns</code> the install script does the following:</p> <ol> <li>Reads <code>dns.host-ip</code> and <code>dns.webhook-port</code> from configuration.</li> <li>Configures the External DNS Helm chart with these values.</li> <li>Reads <code>dns.domain</code> from configuration and gets the IP address of the cluster control plane VM.</li> <li>Configures the running webhook server with these values using REST calls.</li> </ol> <p>Therefore you must edit the <code>config.yaml</code> before installing the Add-On with values that make sense for your environment.</p>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Run the <code>dtk</code> script in the repo root:</p> <pre><code>./dtk cluster create\n</code></pre> <p>The script will use KVM to provision and configure a cluster based on the <code>config.yaml</code> file in the project root resulting in a three node Alma Linux cluster consisting of one controller and two workers. See the Configuration section for details.</p> <p>I recommend you run first with just the <code>check-tools</code> command to check the versions of the tools and utilities used by the scripts (curl, etc.) against the tested versions. E.g.:</p> <pre><code>./dtk check-tools\n</code></pre> <p>The tools / components checked by the script are:</p> Component Purpose Only for Virtual Box? Only for KVM? <code>openssl</code> SSL Connectivity to guests. - - <code>openssh</code> SSL Connectivity to guests. - - <code>genisoimage</code> Builds an ISO to configure VirtualBox. Yes - Virtual Box (vboxmanage) Provisions VirtualBox VMs. Yes - Host operating system I run Ubuntu but there's really nothing specific to any given Linux as long as Bash is available. - - <code>kubectl</code> Connect to the provisioned cluster to run cluster commands. - - <code>curl</code> Download binaries. - - <code>helm</code> Install Add-Ons (e.g.: CoreDNS.) - - <code>yq</code> Parse the configuration yaml file. - - <code>virt-install</code> Provision KVM VMs. - Yes <code>virsh</code> Provision KVM VMs. - Yes <p>You may need to install tools and there will likely be differences between the versions tested by the project, and what you've installed. You have to decide whether the differences are material. Slight version differences may not matter, but for sure you need all the listed tools.</p> <p>Note</p> <p>In keeping with the project philosophy of preserving your desktop environment - you must install the tools listed in order to use this project - the project seeks to not alter your desktop OS.</p> <p>Once the cluster comes up, the script will display a message telling you how to set your <code>KUBECONFIG</code> environment variable in order to access the cluster. It will also display a message showing how to SSH into each node. (There's also a helper script <code>sshto</code> that you can use for that.)</p>"}]}