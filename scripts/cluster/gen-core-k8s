#!/usr/bin/env bash
#
# Generates the Kubernetes core cluster by installing only the canonical
# Kubernetes controller and worker components in each VM based on the VM's
# ordinal position in the cluster. (Node 0 is a controller+worker, Nodes
# >=1 are workers.)
#

set -e

long_opts=containerized-cplane:,kube-proxy-enabled:,priv-key:,ca-cert:,ca-key:,config:,cluster-cidr:

# api
containerized_cplane=
kube_proxy_enabled=
priv_key=
ca_cert=
ca_key=
config=
cluster_cidr=

#
# option parsing helper
#
function opt_val() {
  opt="$1"
  if [[ "$opt" == =* ]]; then
    echo "${opt:1}"
  else
    echo "$opt"
  fi
}

#
# parses command line parameters and sets script variables from them
#
function parse_args() {
  if [[ "$#" -eq 0 ]]; then
    echo "no args provided"
    exit 1
  fi
  local parsed
  local script_name=$(basename "$0")
  parsed=$(getopt --options "" --longoptions $long_opts -n $script_name -- "$@")
  eval set -- "$parsed"
  while true; do
    case "$1" in
      --containerized-cplane)
        containerized_cplane=$(opt_val "$2")
        shift 2
        ;;
      --kube-proxy-enabled)
        kube_proxy_enabled=$(opt_val "$2")
        shift 2
        ;;
      --priv-key)
        priv_key=$(opt_val "$2")
        shift 2
        ;;
      --ca-cert)
        ca_cert=$(opt_val "$2")
        shift 2
        ;;
      --ca-key)
        ca_key=$(opt_val "$2")
        shift 2
        ;;
      --config)
        config=$(opt_val "$2")
        shift 2
        ;;
      --cluster-cidr)
        cluster_cidr=$(opt_val "$2")
        shift 2
        ;;
      --)
        shift
        break
        ;;
    esac
  done
}

parse_args "$@"

# The control plane node will get the worker components for two reasons: 1) it supports
# a containerized control plane, and 2) it lets the controller be also a worker

vmcnt=$(yq '.vms | length' $config)
vm_names=()

for ((i = 0; i < $vmcnt; ++i)); do
  vm_name=$(yq .vms[$i].name $config)
  pod_cidr=$(yq .vms[$i].pod-cidr $config)
  vm_ip=$(xec get-vm-ip $vm_name)

  if [[ $i -eq 0 ]]; then
    controller_ip=$vm_ip
    xec gen-admin-kubeconfig --controller-ip=$controller_ip --ca-cert=$ca_cert --ca-key=$ca_key
    admin_kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
  fi

  ssh -i $priv_key root@$vm_ip mkdir -p /etc/desktop-kubernetes/static-pods

  xec configure-worker\
   --controller-ip=$controller_ip\
   --kube-proxy-enabled=$kube_proxy_enabled\
   --worker-hostname=$vm_name\
   --priv-key=$priv_key\
   --admin-kubeconfig=$admin_kubeconfig\
   --pod-cidr=$pod_cidr\
   --containerized-cplane=$containerized_cplane\
   --ca-cert=$ca_cert\
   --ca-key=$ca_key

  if [[ $i -eq 0 ]]; then
    xec configure-controller\
     --controller-hostname=$vm_name\
     --priv-key=$priv_key\
     --admin-kubeconfig=$admin_kubeconfig\
     --containerized-cplane=$containerized_cplane\
     --ca-cert=$ca_cert\
     --ca-key=$ca_key\
     --cluster-cidr=$cluster_cidr
  fi

  vm_names+=($vm_name)
done

echo "Waiting for all nodes to be Ready"
for ((i = 0; i < $vmcnt; ++i)); do
  kubectl --kubeconfig $admin_kubeconfig wait node "${vm_names[$i]}" --for=condition=Ready --timeout=30s
done

echo "Labeling node(s) - first node is controller & worker, all other nodes are workers"
for ((i = 0; i < $vmcnt; ++i)); do
  if [[ $i -eq 0 ]]; then
    labels=(controller worker)
  else
    labels=(worker)
  fi
  for label in "${labels[@]}"; do
    kubectl --kubeconfig $admin_kubeconfig label node "${vm_names[$i]}" node-role.kubernetes.io/$label=
  done
done

# if kube-proxy enabled, configure routes on each vm to route pod ip addresses
host_only_network=$(yq .vbox.host-only-network $config)

if [[ -n "$host_only_network" ]] && [[ $kube_proxy_enabled -eq 1 ]]; then
  echo "configuring kube-proxy routes"
  for ((i = 0; i < $vmcnt; ++i)); do
    this_vm=$(yq .vms[$i].name $config)
    this_ip=$(xec get-vm-ip $this_vm)
    for ((j = 0; j < $vmcnt; ++j)); do
      if [[ $i -eq $j ]]; then
        continue
      fi
      other_vm=$(yq .vms[$j].name $config)
      other_ip=$(xec get-vm-ip $other_vm)
      other_cidr=$(yq .vms[$j].pod-cidr $config)
      ssh -i $priv_key root@$this_ip "ip route add $other_cidr via $other_ip dev enp0s8"
    done
  done
fi

echo "Core Kubernetes cluster creation successful"
