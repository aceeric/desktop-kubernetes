#!/usr/bin/env bash
#
# Generates the Kubernetes core cluster by installing only the canonical
# Kubernetes controller and worker components in each VM based on the VM's
# ordinal position in the cluster. (Node 0 is a controller+worker, Nodes
# >=1 are workers.)
#

set -e

long_opts=containerized-cplane:,kube-proxy-enabled:,priv-key:,ca-cert:,ca-key:,config:,cluster-cidr:

containerized_cplane=
kube_proxy_enabled=
priv_key=
ca_cert=
ca_key=
config=
cluster_cidr=

if ! parsed=$(xec parseargs $long_opts "$@"); then
  echo "$parsed"
  exit 1
fi
eval $(echo -e "$parsed")

# The control plane node will get the worker components for two reasons: 1) it supports
# a containerized control plane, and 2) it lets the controller be also a worker

vmcnt=$(yq '.vms | length' $config)
vm_names=()

for ((i = 0; i < $vmcnt; ++i)); do
  vm_name=$(yq .vms[$i].name $config)
  pod_cidr=$(yq .vms[$i].pod-cidr $config)
  vm_ip=$(xec get-vm-ip $vm_name)

  if [[ $i -eq 0 ]]; then
    controller_ip=$vm_ip
    xec gen-kubeconfig\
      --subject-org=system:masters\
      --subject-cn=admin\
      --kubeconfig-name=admin\
      --controller-ip=$controller_ip\
      --ca-cert=$ca_cert\
      --ca-key=$ca_key
    admin_kubeconfig=$DTKBASE/generated/kubeconfig/admin.kubeconfig
  fi

  ssh -i $priv_key root@$vm_ip mkdir -p /etc/desktop-kubernetes/static-pods

  xec configure-worker\
   --controller-ip=$controller_ip\
   --kube-proxy-enabled=$kube_proxy_enabled\
   --worker-hostname=$vm_name\
   --priv-key=$priv_key\
   --admin-kubeconfig=$admin_kubeconfig\
   --pod-cidr=$pod_cidr\
   --containerized-cplane=$containerized_cplane\
   --config=$config\
   --ca-cert=$ca_cert\
   --ca-key=$ca_key

  if [[ $i -eq 0 ]]; then
    xec configure-controller\
     --controller-hostname=$vm_name\
     --priv-key=$priv_key\
     --admin-kubeconfig=$admin_kubeconfig\
     --containerized-cplane=$containerized_cplane\
     --ca-cert=$ca_cert\
     --ca-key=$ca_key\
     --cluster-cidr=$cluster_cidr
  fi

  vm_names+=($vm_name)
done

echo "Waiting for all nodes to be Ready"
for ((i = 0; i < $vmcnt; ++i)); do
  kubectl --kubeconfig $admin_kubeconfig wait node "${vm_names[$i]}" --for=condition=Ready --timeout=30s
done

echo "Labeling node(s) - first node is controller & worker, all other nodes are workers"
for ((i = 0; i < $vmcnt; ++i)); do
  if [[ $i -eq 0 ]]; then
    labels=(controller worker)
  else
    labels=(worker)
  fi
  for label in "${labels[@]}"; do
    kubectl --kubeconfig $admin_kubeconfig label node "${vm_names[$i]}" node-role.kubernetes.io/$label=
  done
done

# if kube-proxy enabled, configure routes on each vm to route pod ip addresses
host_only_network=$(yq .vbox.host-only-network $config)

if [[ -n "$host_only_network" ]] && [[ $kube_proxy_enabled -eq 1 ]]; then
  echo "configuring kube-proxy routes"
  for ((i = 0; i < $vmcnt; ++i)); do
    this_vm=$(yq .vms[$i].name $config)
    this_ip=$(xec get-vm-ip $this_vm)
    for ((j = 0; j < $vmcnt; ++j)); do
      if [[ $i -eq $j ]]; then
        continue
      fi
      other_vm=$(yq .vms[$j].name $config)
      other_ip=$(xec get-vm-ip $other_vm)
      other_cidr=$(yq .vms[$j].pod-cidr $config)
      ssh -i $priv_key root@$this_ip "echo $other_cidr via $other_ip dev enp0s8 >> /etc/sysconfig/network-scripts/route-enp0s8"
    done
    ssh -i $priv_key root@$this_ip systemctl restart NetworkManager
  done
fi

echo "Core Kubernetes cluster creation successful"
